{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Vocabulary Building and Data Loading\n",
        "\n",
        "## Learning Objectives\n",
        "- Build vocabulary from cleaned text data\n",
        "- Create efficient PyTorch data loaders\n",
        "- Implement text-to-sequence conversion\n",
        "- Handle variable-length sequences\n",
        "- Understand PyTorch Dataset and DataLoader concepts\n",
        "\n",
        "## Phase 1: PyTorch Fundamentals ðŸ§ \n",
        "*Build everything from scratch to understand the foundations*\n",
        "\n",
        "## Phase 2: Transformers Enhancement ðŸš€\n",
        "*Enhance with modern NLP tools after mastering fundamentals*\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, you'll build the data pipeline that converts your cleaned text into numerical sequences that PyTorch models can process. This involves:\n",
        "\n",
        "1. **Vocabulary Building**: Creating word-to-index mappings\n",
        "2. **Text-to-Sequence Conversion**: Converting text to numerical sequences\n",
        "3. **PyTorch Dataset**: Creating custom dataset classes\n",
        "4. **Data Loading**: Implementing efficient data loaders with batching\n",
        "5. **Pipeline Validation**: Testing and optimizing your data pipeline\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have completed:\n",
        "- âœ… `00_exploration.ipynb` - Data exploration and EDA\n",
        "- âœ… `01_preprocessing.ipynb` - Text cleaning and preprocessing\n",
        "\n",
        "You'll be working with the cleaned datasets from `data/interim/` folder.\n",
        "\n",
        "---\n",
        "\n",
        "## TODO 1: Build Vocabulary\n",
        "\n",
        "**Goal**: Create word-to-index mapping from cleaned text\n",
        "\n",
        "**Steps**:\n",
        "1. Load cleaned text data from `data/interim/`\n",
        "2. Tokenize text into words\n",
        "3. Build vocabulary with special tokens:\n",
        "   - `<PAD>` for padding (index 0)\n",
        "   - `<UNK>` for unknown words (index 1)\n",
        "   - `<START>` and `<END>` tokens (optional)\n",
        "4. Calculate vocabulary statistics\n",
        "\n",
        "**Hint**: Use `Counter` to count word frequencies, set minimum frequency threshold\n",
        "\n",
        "**Expected Output**: Word-to-index mapping and vocabulary size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0            deeds reason earthquake may allah forgive\n",
            "1                   forest fire near ronge sask canada\n",
            "2    residents asked shelter place notified officer...\n",
            "3    thirteenzero people receive wildfires evacuati...\n",
            "4    got sent photo ruby alaska smoke wildfires pou...\n",
            "Name: text_clean, dtype: object\n",
            "object\n",
            "0\n",
            "0\n",
            "0     [deeds, reason, earthquake, may, allah, forgive]\n",
            "1            [forest, fire, near, ronge, sask, canada]\n",
            "2    [residents, asked, shelter, place, notified, o...\n",
            "3    [thirteenzero, people, receive, wildfires, eva...\n",
            "4    [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
            "Name: text_clean, dtype: object\n",
            "[('hundred', 540), ('one', 439), ('two', 435), ('thousand', 417), ('like', 345), ('amp', 298), ('fire', 250), ('get', 229), ('new', 224), ('via', 220)]\n",
            "Vocabulary size: 14644\n",
            "First 10 words: ['UNK', 'aaaa', 'aaaaaaallll', 'aaaaaand', 'aaarrrgghhh', 'aampb', 'aampw', 'aan', 'aannnnd', 'aar', 'aashiqui', 'aba', 'abandon', 'abandoned', 'abandoning', 'abbandoned', 'abbott', 'abbswinston', 'abc', 'abcnews', 'abcs', 'abe', 'aberdeen', 'abes', 'abha']\n",
            "Last 10 words: ['Ã¥Â£one', 'Ã¥Â£sixbn', 'Ã¥Â£three', 'Ã¥Â£two', 'Ã¥Â¬only', 'Ã¥Ã¨mgnafricaÃ¥Â¨', 'Ã¥Ãªfedex', 'Ã¥Ãªi', 'Ã¥Ãªis', 'Ã¬Ã±one']\n"
          ]
        }
      ],
      "source": [
        "# TODO 1: Build vocabulary\n",
        "# Your implementation here\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "cleaned_text = pd.read_csv(\"../data/interim/train_cleaned.csv\")\n",
        "extracted_text = cleaned_text[\"text_clean\"]\n",
        "print(extracted_text.head())\n",
        "print(extracted_text.dtypes)\n",
        "print(extracted_text.isna().sum())\n",
        "extracted_text = cleaned_text[\"text_clean\"].dropna()\n",
        "print(extracted_text.isna().sum())\n",
        "\n",
        "\n",
        "extracted_words = extracted_text.apply(lambda x: x.split())\n",
        "print(extracted_words.head())\n",
        "count_words = Counter(word for words in extracted_words for word in words)\n",
        "print(count_words.most_common(10))\n",
        "vocab = sorted(count_words.keys())\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"First 10 words: {vocab[:25]}\")\n",
        "print(f\"Last 10 words: {vocab[-10:]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 2: Text to Sequence Conversion\n",
        "\n",
        "**Goal**: Convert text to numerical sequences\n",
        "\n",
        "**Steps**:\n",
        "1. Implement function to convert text to sequence of indices\n",
        "2. Handle unknown words with `<UNK>` token\n",
        "3. Add padding to sequences for batch processing\n",
        "4. Test conversion on sample texts\n",
        "\n",
        "**Hint**: Use vocabulary mapping and consider sequence length limits\n",
        "\n",
        "**Expected Output**: Numerical sequences ready for model input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 12702, 13315, 1, 1, 13528, 14280, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# TODO 2: Text to sequence conversion\n",
        "# Your implementation here\n",
        "\n",
        "\n",
        "# Make sure <PAD> is at index 0 and <UNK> is at index 1 in the vocab\n",
        "vocab = ['<PAD>', '<UNK>'] + [word for word in vocab if word not in ('<PAD>', '<UNK>')]\n",
        "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def text_to_sequence(text, vocab_dict, max_length=50, pad_idx=0, unk_idx=1):\n",
        "    \"\"\"\n",
        "    Convert text to sequence of indices using vocabulary mapping.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to convert\n",
        "        vocab_dict (dict): Vocabulary mapping (expects <PAD> at 0, <UNK> at 1)\n",
        "        max_length (int): Maximum sequence length\n",
        "        pad_idx (int): Index for padding token (should be 0)\n",
        "        unk_idx (int): Index for unknown token (should be 1)\n",
        "        \n",
        "    Returns:    \n",
        "        list: Sequence of indices\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase and split into words\n",
        "    words = text.lower().split()\n",
        "    \n",
        "    # Convert words to indices using vocabulary mapping\n",
        "    sequence = [vocab_dict.get(word, unk_idx) for word in words]\n",
        "    \n",
        "    # Pad or truncate sequence to max_length\n",
        "    if len(sequence) < max_length:\n",
        "        sequence.extend([pad_idx] * (max_length - len(sequence)))\n",
        "    else:\n",
        "        sequence = sequence[:max_length]\n",
        "    \n",
        "    return sequence\n",
        "\n",
        "# Test the function\n",
        "test_text = \"This is a test tweet with some unknown words and a disaster.\"\n",
        "print(text_to_sequence(test_text, vocab_dict))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 3: Custom PyTorch Dataset\n",
        "\n",
        "**Goal**: Create PyTorch Dataset class for disaster tweets\n",
        "\n",
        "**Steps**:\n",
        "1. Inherit from `torch.utils.data.Dataset`\n",
        "2. Implement `__len__` and `__getitem__` methods\n",
        "3. Handle text-to-sequence conversion\n",
        "4. Return tensors for text and labels\n",
        "\n",
        "**Hint**: Use `torch.tensor()` for tensor creation\n",
        "\n",
        "**Expected Output**: Custom dataset class ready for DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Custom PyTorch dataset\n",
        "# Your implementation here\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DisasterTweetsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab_dict, max_length=50):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 4: Data Loading and Batching\n",
        "\n",
        "**Goal**: Create efficient data loaders with proper batching\n",
        "\n",
        "**Steps**:\n",
        "1. Create train/validation/test splits\n",
        "2. Implement custom collate function for variable-length sequences\n",
        "3. Create DataLoaders with appropriate batch sizes\n",
        "4. Test data loading pipeline\n",
        "\n",
        "**Hint**: Use `torch.nn.utils.rnn.pad_sequence` for padding\n",
        "\n",
        "**Expected Output**: Efficient data loading pipeline ready for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4: Data loading and batching\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 5: Vocabulary Analysis and Optimization\n",
        "\n",
        "**Goal**: Analyze vocabulary characteristics and optimize for modeling\n",
        "\n",
        "**Steps**:\n",
        "1. Calculate vocabulary statistics (size, coverage, frequency distribution)\n",
        "2. Analyze sequence length distribution\n",
        "3. Determine optimal sequence length for padding/truncation\n",
        "4. Visualize vocabulary and sequence statistics\n",
        "5. Save vocabulary for later use\n",
        "\n",
        "**Hint**: Consider vocabulary size impact on model performance and memory usage\n",
        "\n",
        "**Expected Output**: Optimized vocabulary ready for model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5: Vocabulary analysis and optimization\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 6: Data Pipeline Testing and Validation\n",
        "\n",
        "**Goal**: Validate the complete data pipeline before moving to modeling\n",
        "\n",
        "**Steps**:\n",
        "1. Test data loading with sample batches\n",
        "2. Verify tensor shapes and data types\n",
        "3. Check for any data inconsistencies\n",
        "4. Measure data loading performance\n",
        "5. Document pipeline characteristics\n",
        "\n",
        "**Hint**: Use `torch.utils.data.DataLoader` with `num_workers` for efficiency\n",
        "\n",
        "**Expected Output**: Validated data pipeline ready for model training\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Transformers Enhancement\n",
        "\n",
        "*After completing Phase 1, consider these enhancements:*\n",
        "\n",
        "- Use HuggingFace tokenizers for consistent tokenization\n",
        "- Leverage pre-trained vocabularies (BERT, RoBERTa)\n",
        "- Compare custom vocabulary vs. pre-trained tokenizers\n",
        "- Analyze vocabulary size impact on model performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6: Data pipeline testing and validation\n",
        "# Your implementation here\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
