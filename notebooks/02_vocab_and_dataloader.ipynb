{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Vocabulary Building and Data Loading\n",
        "\n",
        "## Learning Objectives\n",
        "- Build vocabulary from cleaned text data\n",
        "- Create efficient PyTorch data loaders\n",
        "- Implement text-to-sequence conversion\n",
        "- Handle variable-length sequences\n",
        "- Understand PyTorch Dataset and DataLoader concepts\n",
        "\n",
        "## Phase 1: PyTorch Fundamentals ðŸ§ \n",
        "*Build everything from scratch to understand the foundations*\n",
        "\n",
        "## Phase 2: Transformers Enhancement ðŸš€\n",
        "*Enhance with modern NLP tools after mastering fundamentals*\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, you'll build the data pipeline that converts your cleaned text into numerical sequences that PyTorch models can process. This involves:\n",
        "\n",
        "1. **Vocabulary Building**: Creating word-to-index mappings\n",
        "2. **Text-to-Sequence Conversion**: Converting text to numerical sequences\n",
        "3. **PyTorch Dataset**: Creating custom dataset classes\n",
        "4. **Data Loading**: Implementing efficient data loaders with batching\n",
        "5. **Pipeline Validation**: Testing and optimizing your data pipeline\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have completed:\n",
        "- âœ… `00_exploration.ipynb` - Data exploration and EDA\n",
        "- âœ… `01_preprocessing.ipynb` - Text cleaning and preprocessing\n",
        "\n",
        "You'll be working with the cleaned datasets from `data/interim/` folder.\n",
        "\n",
        "---\n",
        "\n",
        "## TODO 1: Build Vocabulary\n",
        "\n",
        "**Goal**: Create word-to-index mapping from cleaned text\n",
        "\n",
        "**Steps**:\n",
        "1. Load cleaned text data from `data/interim/`\n",
        "2. Tokenize text into words\n",
        "3. Build vocabulary with special tokens:\n",
        "   - `<PAD>` for padding (index 0)\n",
        "   - `<UNK>` for unknown words (index 1)\n",
        "   - `<START>` and `<END>` tokens (optional)\n",
        "4. Calculate vocabulary statistics\n",
        "\n",
        "**Hint**: Use `Counter` to count word frequencies, set minimum frequency threshold\n",
        "\n",
        "**Expected Output**: Word-to-index mapping and vocabulary size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Build vocabulary\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 2: Text to Sequence Conversion\n",
        "\n",
        "**Goal**: Convert text to numerical sequences\n",
        "\n",
        "**Steps**:\n",
        "1. Implement function to convert text to sequence of indices\n",
        "2. Handle unknown words with `<UNK>` token\n",
        "3. Add padding to sequences for batch processing\n",
        "4. Test conversion on sample texts\n",
        "\n",
        "**Hint**: Use vocabulary mapping and consider sequence length limits\n",
        "\n",
        "**Expected Output**: Numerical sequences ready for model input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: Text to sequence conversion\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 3: Custom PyTorch Dataset\n",
        "\n",
        "**Goal**: Create PyTorch Dataset class for disaster tweets\n",
        "\n",
        "**Steps**:\n",
        "1. Inherit from `torch.utils.data.Dataset`\n",
        "2. Implement `__len__` and `__getitem__` methods\n",
        "3. Handle text-to-sequence conversion\n",
        "4. Return tensors for text and labels\n",
        "\n",
        "**Hint**: Use `torch.tensor()` for tensor creation\n",
        "\n",
        "**Expected Output**: Custom dataset class ready for DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Custom PyTorch dataset\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 4: Data Loading and Batching\n",
        "\n",
        "**Goal**: Create efficient data loaders with proper batching\n",
        "\n",
        "**Steps**:\n",
        "1. Create train/validation/test splits\n",
        "2. Implement custom collate function for variable-length sequences\n",
        "3. Create DataLoaders with appropriate batch sizes\n",
        "4. Test data loading pipeline\n",
        "\n",
        "**Hint**: Use `torch.nn.utils.rnn.pad_sequence` for padding\n",
        "\n",
        "**Expected Output**: Efficient data loading pipeline ready for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4: Data loading and batching\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 5: Vocabulary Analysis and Optimization\n",
        "\n",
        "**Goal**: Analyze vocabulary characteristics and optimize for modeling\n",
        "\n",
        "**Steps**:\n",
        "1. Calculate vocabulary statistics (size, coverage, frequency distribution)\n",
        "2. Analyze sequence length distribution\n",
        "3. Determine optimal sequence length for padding/truncation\n",
        "4. Visualize vocabulary and sequence statistics\n",
        "5. Save vocabulary for later use\n",
        "\n",
        "**Hint**: Consider vocabulary size impact on model performance and memory usage\n",
        "\n",
        "**Expected Output**: Optimized vocabulary ready for model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5: Vocabulary analysis and optimization\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 6: Data Pipeline Testing and Validation\n",
        "\n",
        "**Goal**: Validate the complete data pipeline before moving to modeling\n",
        "\n",
        "**Steps**:\n",
        "1. Test data loading with sample batches\n",
        "2. Verify tensor shapes and data types\n",
        "3. Check for any data inconsistencies\n",
        "4. Measure data loading performance\n",
        "5. Document pipeline characteristics\n",
        "\n",
        "**Hint**: Use `torch.utils.data.DataLoader` with `num_workers` for efficiency\n",
        "\n",
        "**Expected Output**: Validated data pipeline ready for model training\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Transformers Enhancement\n",
        "\n",
        "*After completing Phase 1, consider these enhancements:*\n",
        "\n",
        "- Use HuggingFace tokenizers for consistent tokenization\n",
        "- Leverage pre-trained vocabularies (BERT, RoBERTa)\n",
        "- Compare custom vocabulary vs. pre-trained tokenizers\n",
        "- Analyze vocabulary size impact on model performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 6: Data pipeline testing and validation\n",
        "# Your implementation here\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
