{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71a8088",
   "metadata": {},
   "source": [
    "# üß† Model Baseline - Building Your First NLP Neural Network\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "In this notebook, you'll build your **first PyTorch neural network for NLP** from scratch! You'll implement a simple but effective baseline model using:\n",
    "- **Embedding Layer**: Convert word indices to dense vectors\n",
    "- **Neural Network Layers**: Learn patterns from text\n",
    "- **Output Layer**: Binary classification (disaster vs. non-disaster)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "1. **Understand word embeddings** and how they represent text\n",
    "2. **Build a PyTorch model** with proper architecture\n",
    "3. **Implement forward pass** for text classification\n",
    "4. **Learn about model components**: Embedding, Linear layers, activations\n",
    "5. **Initialize model parameters** properly\n",
    "6. **Understand model architecture decisions** and their trade-offs\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, ensure you've completed:\n",
    "- ‚úÖ `00_exploration.ipynb` - Data exploration\n",
    "- ‚úÖ `01_preprocessing.ipynb` - Text preprocessing\n",
    "- ‚úÖ `02_vocab_and_dataloader.ipynb` - Vocabulary and DataLoader\n",
    "\n",
    "You should have:\n",
    "- `vocab_dict`: Vocabulary mapping words to indices\n",
    "- `train_loader` and `val_loader`: DataLoaders ready\n",
    "- Understanding of your data shapes: `[batch_size, seq_length]`\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Model Architecture Overview\n",
    "\n",
    "We'll build a **simple baseline model** with this structure:\n",
    "\n",
    "```\n",
    "Input: [batch_size, seq_length] (word indices)\n",
    "    ‚Üì\n",
    "Embedding Layer: [batch_size, seq_length, embedding_dim]\n",
    "    ‚Üì\n",
    "Pooling/Aggregation: [batch_size, embedding_dim]\n",
    "    ‚Üì\n",
    "Hidden Layer(s): [batch_size, hidden_dim]\n",
    "    ‚Üì\n",
    "Output Layer: [batch_size, 1]\n",
    "    ‚Üì\n",
    "Sigmoid: [batch_size, 1] (probability)\n",
    "```\n",
    "\n",
    "### Key Design Decisions:\n",
    "- **Embedding Dimension**: How many features per word? (e.g., 100, 200, 300)\n",
    "- **Pooling Strategy**: How to combine word embeddings? (mean, max, sum)\n",
    "- **Hidden Layers**: How many? How large?\n",
    "- **Activation Functions**: ReLU, tanh, or others?\n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Build!\n",
    "\n",
    "Work through each TODO step-by-step. **Write the code yourself** - no copy-paste!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089685ea",
   "metadata": {},
   "source": [
    "## TODO 1: Setup and Load Dependencies üì¶\n",
    "\n",
    "**Goal**: Import necessary libraries and load your vocabulary and data.\n",
    "\n",
    "**What you need**:\n",
    "- PyTorch modules: `torch`, `torch.nn`, `torch.nn.functional`\n",
    "- Data handling: `pandas`, `numpy`\n",
    "- Your previous work: vocabulary dictionary, DataLoaders\n",
    "\n",
    "**Hint**: You'll need to re-run or import the code from `02_vocab_and_dataloader.ipynb` to get your `vocab_dict`, `train_loader`, and create a validation loader.\n",
    "\n",
    "**Questions to consider**:\n",
    "- What is the vocabulary size? (You'll need this for the embedding layer)\n",
    "- What device will you use? (CPU vs. GPU)\n",
    "- Do you need to set a random seed for reproducibility?\n",
    "\n",
    "**Expected outcome**: \n",
    "- All imports successful\n",
    "- `vocab_dict` loaded\n",
    "- `train_loader` ready\n",
    "- Know the `vocab_size` and `max_seq_length`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Your code here\n",
    "# Import libraries\n",
    "\n",
    "\n",
    "# Load vocabulary and create DataLoaders\n",
    "\n",
    "\n",
    "# Set device and random seed\n",
    "\n",
    "\n",
    "# Print key information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af9266",
   "metadata": {},
   "source": [
    "## TODO 2: Understand Word Embeddings üî§‚û°Ô∏èüî¢\n",
    "\n",
    "**Goal**: Learn what embeddings are and why we use them.\n",
    "\n",
    "**Key Concepts**:\n",
    "1. **One-Hot Encoding Problem**: If vocab_size = 15,000, each word would be a 15,000-dimensional sparse vector ‚ùå\n",
    "2. **Dense Embeddings Solution**: Each word becomes a dense vector of size `embedding_dim` (e.g., 100) ‚úÖ\n",
    "3. **Learned Representations**: The model learns meaningful word vectors during training\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# One-hot encoding (sparse)\n",
    "word \"fire\" ‚Üí [0, 0, 0, ..., 1, ..., 0]  # 15,000 dimensions, one 1\n",
    "\n",
    "# Embedding (dense)\n",
    "word \"fire\" ‚Üí [0.23, -0.45, 0.67, ...]  # 100 dimensions, all meaningful\n",
    "```\n",
    "\n",
    "**Why embeddings?**:\n",
    "- **Efficiency**: 100 dimensions vs. 15,000 dimensions\n",
    "- **Generalization**: Similar words get similar vectors\n",
    "- **Learnable**: Vectors are updated during training\n",
    "\n",
    "**Task**: Create a simple example to understand `nn.Embedding`:\n",
    "- Create a small embedding layer (vocab_size=10, embedding_dim=5)\n",
    "- Pass in some word indices\n",
    "- Observe the output shape and values\n",
    "\n",
    "**Hint**: `nn.Embedding(num_embeddings, embedding_dim)` creates a lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72712b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Your code here\n",
    "# Create a simple embedding example to understand how it works\n",
    "\n",
    "# Create embedding layer\n",
    "\n",
    "\n",
    "# Create sample input (word indices)\n",
    "\n",
    "\n",
    "# Pass through embedding\n",
    "\n",
    "\n",
    "# Print shapes and observe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c229a",
   "metadata": {},
   "source": [
    "## TODO 3: Design Your Model Architecture üèóÔ∏è\n",
    "\n",
    "**Goal**: Create a PyTorch model class for text classification.\n",
    "\n",
    "**Model Components**:\n",
    "1. **Embedding Layer**: `nn.Embedding(vocab_size, embedding_dim, padding_idx=0)`\n",
    "2. **Pooling/Aggregation**: How to convert [batch, seq_len, embed_dim] ‚Üí [batch, embed_dim]\n",
    "   - Mean pooling: Take average of all word vectors\n",
    "   - Max pooling: Take maximum values\n",
    "   - Sum pooling: Sum all word vectors\n",
    "3. **Hidden Layers**: `nn.Linear(input_dim, hidden_dim)` with activation (ReLU)\n",
    "4. **Output Layer**: `nn.Linear(hidden_dim, 1)` for binary classification\n",
    "5. **Dropout**: `nn.Dropout(p=0.5)` for regularization\n",
    "\n",
    "**Architecture Template**:\n",
    "```python\n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "        # Define forward pass\n",
    "        # Return: [batch_size, 1]\n",
    "```\n",
    "\n",
    "**Design Decisions**:\n",
    "- `embedding_dim`: Start with 100 or 200\n",
    "- `hidden_dim`: Try 128 or 256\n",
    "- Pooling: Mean pooling is a good start (handles variable lengths well)\n",
    "- Dropout: 0.3-0.5 to prevent overfitting\n",
    "\n",
    "**Hint**: The forward pass should:\n",
    "1. Apply embedding\n",
    "2. Pool across sequence length dimension\n",
    "3. Pass through hidden layer(s) with activation\n",
    "4. Apply dropout\n",
    "5. Output layer for final prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Your code here\n",
    "# Create your model class\n",
    "\n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # TODO: Define your layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # x shape: [batch_size, seq_length]\n",
    "        \n",
    "        # Step 1: Embedding\n",
    "        \n",
    "        # Step 2: Pooling (mean, max, or sum)\n",
    "        \n",
    "        # Step 3: Hidden layer(s) with activation\n",
    "        \n",
    "        # Step 4: Dropout\n",
    "        \n",
    "        # Step 5: Output layer\n",
    "        \n",
    "        return output  # shape: [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc962966",
   "metadata": {},
   "source": [
    "## TODO 4: Instantiate and Inspect Your Model üîç\n",
    "\n",
    "**Goal**: Create an instance of your model and understand its structure.\n",
    "\n",
    "**Tasks**:\n",
    "1. **Instantiate the model** with your chosen hyperparameters\n",
    "2. **Move model to device** (CPU or GPU)\n",
    "3. **Print model architecture** using `print(model)`\n",
    "4. **Count parameters** to understand model size\n",
    "5. **Test forward pass** with a dummy batch\n",
    "\n",
    "**Counting Parameters**:\n",
    "```python\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "\n",
    "**Expected hyperparameters**:\n",
    "- `vocab_size`: Your actual vocabulary size (from TODO 1)\n",
    "- `embedding_dim`: 100-300 (start with 100 or 200)\n",
    "- `hidden_dim`: 128-512 (start with 128 or 256)\n",
    "- `padding_idx`: 0 (your `<PAD>` token index)\n",
    "\n",
    "**Test with dummy data**:\n",
    "- Create a dummy batch: `torch.randint(0, vocab_size, (32, 50))`\n",
    "- Pass through model\n",
    "- Check output shape: should be `[32, 1]`\n",
    "\n",
    "**Questions to answer**:\n",
    "- How many trainable parameters does your model have?\n",
    "- What's the shape of the embedding layer weight matrix?\n",
    "- Does your model work with different batch sizes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec210260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Your code here\n",
    "# Instantiate and inspect your model\n",
    "\n",
    "# Define helper function\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "\n",
    "\n",
    "# Move to device\n",
    "\n",
    "\n",
    "# Print model architecture\n",
    "\n",
    "\n",
    "# Count and print parameters\n",
    "\n",
    "\n",
    "# Test forward pass with dummy data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06aac7",
   "metadata": {},
   "source": [
    "## TODO 5: Understand Loss Function and Optimizer ‚öôÔ∏è\n",
    "\n",
    "**Goal**: Choose and configure loss function and optimizer for binary classification.\n",
    "\n",
    "### Loss Function: Binary Cross-Entropy\n",
    "\n",
    "For binary classification, we use **Binary Cross-Entropy (BCE) Loss**:\n",
    "\n",
    "```python\n",
    "# Two options:\n",
    "# 1. BCELoss - requires sigmoid in model output\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 2. BCEWithLogitsLoss - includes sigmoid (more numerically stable) ‚úÖ RECOMMENDED\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "```\n",
    "\n",
    "**Why BCEWithLogitsLoss?**\n",
    "- More numerically stable (combines sigmoid + BCE)\n",
    "- Less prone to gradient issues\n",
    "- Standard choice for binary classification\n",
    "\n",
    "### Optimizer: Adam\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) is a great default choice:\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "**Learning Rate Guidelines**:\n",
    "- Start with `lr=0.001` (standard default)\n",
    "- Too high ‚Üí unstable training, loss explodes\n",
    "- Too low ‚Üí very slow training\n",
    "- You can adjust later based on training behavior\n",
    "\n",
    "### Optional: Learning Rate Scheduler\n",
    "\n",
    "Reduce learning rate as training progresses:\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "```\n",
    "\n",
    "**Task**: Set up your loss function and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Your code here\n",
    "# Set up loss function and optimizer\n",
    "\n",
    "# Define loss function\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "\n",
    "\n",
    "# Optional: Learning rate scheduler\n",
    "\n",
    "\n",
    "# Print configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caa00c",
   "metadata": {},
   "source": [
    "## TODO 6: Test Model with Real Data üß™\n",
    "\n",
    "**Goal**: Verify your model works with actual data from your DataLoader.\n",
    "\n",
    "**Tasks**:\n",
    "1. Get one batch from your `train_loader`\n",
    "2. Pass batch through model\n",
    "3. Calculate loss\n",
    "4. Check all shapes are correct\n",
    "5. Verify gradients can be computed\n",
    "\n",
    "**Expected Flow**:\n",
    "```python\n",
    "# Get batch\n",
    "texts, labels = next(iter(train_loader))\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(texts)\n",
    "\n",
    "# Calculate loss\n",
    "loss = loss_fn(outputs, labels)\n",
    "\n",
    "# Backward pass (just to test)\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "**Shapes to verify**:\n",
    "- `texts`: `[batch_size, seq_length]` (e.g., `[32, 50]`)\n",
    "- `labels`: `[batch_size]` (e.g., `[32]`)\n",
    "- `outputs`: `[batch_size, 1]` (e.g., `[32, 1]`)\n",
    "- `loss`: scalar value\n",
    "\n",
    "**Important**: If using `BCEWithLogitsLoss`, you need to:\n",
    "- Ensure labels are float: `labels.float()`\n",
    "- Reshape output if needed: `outputs.squeeze()` or `labels.unsqueeze(1)`\n",
    "\n",
    "**What to check**:\n",
    "- ‚úÖ No shape errors\n",
    "- ‚úÖ Loss is a reasonable number (not NaN, not infinity)\n",
    "- ‚úÖ Gradients are computed\n",
    "- ‚úÖ Model parameters require gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Your code here\n",
    "# Test model with real data\n",
    "\n",
    "# Get one batch\n",
    "\n",
    "\n",
    "# Move to device if needed\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "\n",
    "\n",
    "# Print shapes and values\n",
    "\n",
    "\n",
    "# Test backward pass\n",
    "\n",
    "\n",
    "# Check gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed27ba3",
   "metadata": {},
   "source": [
    "## TODO 7: Save Your Model Architecture üíæ\n",
    "\n",
    "**Goal**: Save your model architecture and configuration for the next notebook.\n",
    "\n",
    "You'll need these for training:\n",
    "- Model class definition\n",
    "- Hyperparameters (vocab_size, embedding_dim, hidden_dim)\n",
    "- Model instance\n",
    "- Loss function\n",
    "- Optimizer\n",
    "\n",
    "**Options for saving**:\n",
    "\n",
    "### Option 1: Save just the class definition\n",
    "Create a `src/models/baseline_model.py` file with your model class\n",
    "\n",
    "### Option 2: Save model state dict\n",
    "```python\n",
    "torch.save(model.state_dict(), 'models/baseline_model.pth')\n",
    "```\n",
    "\n",
    "### Option 3: Save entire model\n",
    "```python\n",
    "torch.save(model, 'models/baseline_model_full.pth')\n",
    "```\n",
    "\n",
    "**Recommended approach** for this learning project:\n",
    "- Copy your model class to `src/models/baseline_model.py`\n",
    "- Save hyperparameters in a config dictionary\n",
    "- This way you can import it in the next notebook for training\n",
    "\n",
    "**Task**: \n",
    "1. Create the model file in `src/models/`\n",
    "2. Save your hyperparameters\n",
    "3. Test that you can reload everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Your code here\n",
    "# Save model architecture and configuration\n",
    "\n",
    "# Create config dictionary\n",
    "\n",
    "\n",
    "# Option 1: Write model class to file (recommended for learning)\n",
    "# You can manually create src/models/baseline_model.py and copy your class there\n",
    "\n",
    "# Option 2: Save model state\n",
    "# Uncomment if you want to save the initialized model\n",
    "\n",
    "\n",
    "# Test reload\n",
    "# from src.models.baseline_model import DisasterTweetClassifier\n",
    "# loaded_model = DisasterTweetClassifier(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896b370",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Built your first PyTorch NLP model from scratch\n",
    "- ‚úÖ Understood word embeddings and why they're used\n",
    "- ‚úÖ Designed a neural network architecture for text classification\n",
    "- ‚úÖ Set up loss function and optimizer\n",
    "- ‚úÖ Tested your model with real data\n",
    "- ‚úÖ Prepared everything for training\n",
    "\n",
    "## üìä Model Summary\n",
    "\n",
    "Review what you've created:\n",
    "- **Architecture**: Embedding ‚Üí Pooling ‚Üí Hidden Layer(s) ‚Üí Output\n",
    "- **Parameters**: ~XXX,XXX trainable parameters\n",
    "- **Input**: Word indices `[batch_size, seq_length]`\n",
    "- **Output**: Binary predictions `[batch_size, 1]`\n",
    "- **Loss**: Binary Cross-Entropy\n",
    "- **Optimizer**: Adam\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Your model is ready! Move on to:\n",
    "- **`04_training_and_eval.ipynb`**: Train your model and evaluate performance\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Learnings\n",
    "\n",
    "**What you learned**:\n",
    "1. **Embeddings convert discrete words to continuous vectors** - more efficient than one-hot\n",
    "2. **PyTorch models are classes** inheriting from `nn.Module`\n",
    "3. **Pooling aggregates variable-length sequences** into fixed-size representations\n",
    "4. **BCEWithLogitsLoss is the standard** for binary classification\n",
    "5. **Adam optimizer is a great default** for most deep learning tasks\n",
    "\n",
    "**Questions to reflect on**:\n",
    "- Why do we need pooling? (Variable length ‚Üí Fixed length for FC layers)\n",
    "- What's the difference between BCE and BCEWithLogitsLoss? (Numerical stability)\n",
    "- How many parameters does your embedding layer have? (vocab_size √ó embedding_dim)\n",
    "- Could you add more hidden layers? (Yes! Deeper networks learn more complex patterns)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
