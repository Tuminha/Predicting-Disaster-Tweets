{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71a8088",
   "metadata": {},
   "source": [
    "# üß† Model Baseline - Building Your First NLP Neural Network\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "In this notebook, you'll build your **first PyTorch neural network for NLP** from scratch! You'll implement a simple but effective baseline model using:\n",
    "- **Embedding Layer**: Convert word indices to dense vectors\n",
    "- **Neural Network Layers**: Learn patterns from text\n",
    "- **Output Layer**: Binary classification (disaster vs. non-disaster)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "1. **Understand word embeddings** and how they represent text\n",
    "2. **Build a PyTorch model** with proper architecture\n",
    "3. **Implement forward pass** for text classification\n",
    "4. **Learn about model components**: Embedding, Linear layers, activations\n",
    "5. **Initialize model parameters** properly\n",
    "6. **Understand model architecture decisions** and their trade-offs\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, ensure you've completed:\n",
    "- ‚úÖ `00_exploration.ipynb` - Data exploration\n",
    "- ‚úÖ `01_preprocessing.ipynb` - Text preprocessing\n",
    "- ‚úÖ `02_vocab_and_dataloader.ipynb` - Vocabulary and DataLoader\n",
    "\n",
    "You should have:\n",
    "- `vocab_dict`: Vocabulary mapping words to indices\n",
    "- `train_loader` and `val_loader`: DataLoaders ready\n",
    "- Understanding of your data shapes: `[batch_size, seq_length]`\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Model Architecture Overview\n",
    "\n",
    "We'll build a **simple baseline model** with this structure:\n",
    "\n",
    "```\n",
    "Input: [batch_size, seq_length] (word indices)\n",
    "    ‚Üì\n",
    "Embedding Layer: [batch_size, seq_length, embedding_dim]\n",
    "    ‚Üì\n",
    "Pooling/Aggregation: [batch_size, embedding_dim]\n",
    "    ‚Üì\n",
    "Hidden Layer(s): [batch_size, hidden_dim]\n",
    "    ‚Üì\n",
    "Output Layer: [batch_size, 1]\n",
    "    ‚Üì\n",
    "Sigmoid: [batch_size, 1] (probability)\n",
    "```\n",
    "\n",
    "### Key Design Decisions:\n",
    "- **Embedding Dimension**: How many features per word? (e.g., 100, 200, 300)\n",
    "- **Pooling Strategy**: How to combine word embeddings? (mean, max, sum)\n",
    "- **Hidden Layers**: How many? How large?\n",
    "- **Activation Functions**: ReLU, tanh, or others?\n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Build!\n",
    "\n",
    "Work through each TODO step-by-step. **Write the code yourself** - no copy-paste!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089685ea",
   "metadata": {},
   "source": [
    "## TODO 1: Setup and Load Dependencies üì¶\n",
    "\n",
    "**Goal**: Import necessary libraries and load your vocabulary and data.\n",
    "\n",
    "**What you need**:\n",
    "- PyTorch modules: `torch`, `torch.nn`, `torch.nn.functional`\n",
    "- Data handling: `pandas`, `numpy`\n",
    "- Your previous work: vocabulary dictionary, DataLoaders\n",
    "\n",
    "**Hint**: You'll need to re-run or import the code from `02_vocab_and_dataloader.ipynb` to get your `vocab_dict`, `train_loader`, and create a validation loader.\n",
    "\n",
    "**Questions to consider**:\n",
    "- What is the vocabulary size? (You'll need this for the embedding layer)\n",
    "- What device will you use? (CPU vs. GPU)\n",
    "- Do you need to set a random seed for reproducibility?\n",
    "\n",
    "**Expected outcome**: \n",
    "- All imports successful\n",
    "- `vocab_dict` loaded\n",
    "- `train_loader` ready\n",
    "- Know the `vocab_size` and `max_seq_length`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb5b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary size: 3160\n",
      "Loaded 7613 training samples\n",
      "0            deeds reason earthquake may allah forgive\n",
      "1                   forest fire near ronge sask canada\n",
      "2    residents asked shelter place notified officer...\n",
      "3    thirteenzero people receive wildfires evacuati...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "Name: text_clean, dtype: object\n",
      "Missing values: 0\n",
      "Test sequence: [1, 1, 1, 1134, 881, 1, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "==================================================\n",
      "Setup Complete!\n",
      "==================================================\n",
      "Vocabulary size: 3160\n",
      "Train samples: 6090\n",
      "Val samples: 1523\n",
      "Train batches: 191\n",
      "Val batches: 48\n",
      "Device: cpu\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO 1: Your code here\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load vocabulary (ALREADY BUILT in notebook 02!)\n",
    "vocab_dict = pickle.load(open('../data/processed/vocab_dict.pkl', 'rb'))\n",
    "vocab_size = len(vocab_dict)\n",
    "print(f\"Loaded vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Load cleaned data\n",
    "cleaned_text = pd.read_csv(\"../data/interim/train_cleaned.csv\")\n",
    "print(f\"Loaded {len(cleaned_text)} training samples\")\n",
    "print(cleaned_text['text_clean'].head())\n",
    "\n",
    "# Check for NaN\n",
    "print(f\"Missing values: {cleaned_text['text_clean'].isna().sum()}\")\n",
    "\n",
    "# Define text_to_sequence function (keep this!)\n",
    "def text_to_sequence(text, vocab_dict, max_length=50, pad_idx=0, unk_idx=1):\n",
    "    \"\"\"Convert text to sequence of indices.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    sequence = [vocab_dict.get(word, unk_idx) for word in words]\n",
    "    \n",
    "    if len(sequence) < max_length:\n",
    "        sequence.extend([pad_idx] * (max_length - len(sequence)))\n",
    "    else:\n",
    "        sequence = sequence[:max_length]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Test it\n",
    "test_text = \"This is a test tweet with disaster\"\n",
    "print(f\"Test sequence: {text_to_sequence(test_text, vocab_dict)}\")\n",
    "\n",
    "# Define Dataset class (keep this!)\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab_dict, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        return text, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create train/val split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    cleaned_text[\"text_clean\"],\n",
    "    cleaned_text[\"target\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=cleaned_text[\"target\"]  # Keep class balance!\n",
    ")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch, vocab_dict, max_length):\n",
    "    texts, labels = zip(*batch)\n",
    "    sequences = [text_to_sequence(text, vocab_dict, max_length) for text in texts]\n",
    "    sequences = torch.tensor(sequences)\n",
    "    return sequences, torch.tensor(labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    DisasterTweetsDataset(train_texts, train_labels, vocab_dict, max_length=50),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate_fn(batch, vocab_dict, max_length=50)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    DisasterTweetsDataset(val_texts, val_labels, vocab_dict, max_length=50),\n",
    "    batch_size=32,\n",
    "    shuffle=False,  # Don't shuffle validation!\n",
    "    collate_fn=lambda batch: collate_fn(batch, vocab_dict, max_length=50)\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Setup Complete!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Val samples: {len(val_texts)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af9266",
   "metadata": {},
   "source": [
    "## TODO 2: Understand Word Embeddings üî§‚û°Ô∏èüî¢\n",
    "\n",
    "**Goal**: Learn what embeddings are and why we use them.\n",
    "\n",
    "**Key Concepts**:\n",
    "1. **One-Hot Encoding Problem**: If vocab_size = 15,000, each word would be a 15,000-dimensional sparse vector ‚ùå\n",
    "2. **Dense Embeddings Solution**: Each word becomes a dense vector of size `embedding_dim` (e.g., 100) ‚úÖ\n",
    "3. **Learned Representations**: The model learns meaningful word vectors during training\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# One-hot encoding (sparse)\n",
    "word \"fire\" ‚Üí [0, 0, 0, ..., 1, ..., 0]  # 15,000 dimensions, one 1\n",
    "\n",
    "# Embedding (dense)\n",
    "word \"fire\" ‚Üí [0.23, -0.45, 0.67, ...]  # 100 dimensions, all meaningful\n",
    "```\n",
    "\n",
    "**Why embeddings?**:\n",
    "- **Efficiency**: 100 dimensions vs. 15,000 dimensions\n",
    "- **Generalization**: Similar words get similar vectors\n",
    "- **Learnable**: Vectors are updated during training\n",
    "\n",
    "**Task**: Create a simple example to understand `nn.Embedding`:\n",
    "- Create a small embedding layer (vocab_size=10, embedding_dim=5)\n",
    "- Pass in some word indices\n",
    "- Observe the output shape and values\n",
    "\n",
    "**Hint**: `nn.Embedding(num_embeddings, embedding_dim)` creates a lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72712b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word indices: tensor([1, 2, 3, 4, 5])\n",
      "Embedded output shape: torch.Size([5, 5])\n",
      "Embedded output values: tensor([[ 1.4211, -0.1007, -0.0907, -0.1990, -0.8617],\n",
      "        [ 0.2942, -0.7440, -0.8582, -0.9711, -2.5935],\n",
      "        [ 0.0091, -0.1182,  0.2968,  0.7337,  0.7997],\n",
      "        [-2.0619,  0.1753, -1.1625, -1.1560,  0.1963],\n",
      "        [ 0.2400, -1.0057,  0.2395, -0.4996,  1.2048]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Your code here\n",
    "# Create a simple embedding example to understand how it works\n",
    "# Create embedding layer\n",
    "embedding_example = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
    "\n",
    "# Create sample input (word indices)\n",
    "word_indices = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Pass through embedding\n",
    "embedded = embedding_example(word_indices)\n",
    "\n",
    "# Print shapes and observe\n",
    "print(f\"Input word indices: {word_indices}\")\n",
    "print(f\"Embedded output shape: {embedded.shape}\")\n",
    "print(f\"Embedded output values: {embedded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c229a",
   "metadata": {},
   "source": [
    "## TODO 3: Design Your Model Architecture üèóÔ∏è\n",
    "\n",
    "**Goal**: Create a PyTorch model class for text classification.\n",
    "\n",
    "**Model Components**:\n",
    "1. **Embedding Layer**: `nn.Embedding(vocab_size, embedding_dim, padding_idx=0)`\n",
    "2. **Pooling/Aggregation**: How to convert [batch, seq_len, embed_dim] ‚Üí [batch, embed_dim]\n",
    "   - Mean pooling: Take average of all word vectors\n",
    "   - Max pooling: Take maximum values\n",
    "   - Sum pooling: Sum all word vectors\n",
    "3. **Hidden Layers**: `nn.Linear(input_dim, hidden_dim)` with activation (ReLU)\n",
    "4. **Output Layer**: `nn.Linear(hidden_dim, 1)` for binary classification\n",
    "5. **Dropout**: `nn.Dropout(p=0.5)` for regularization\n",
    "\n",
    "**Architecture Template**:\n",
    "```python\n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length]\n",
    "        # Define forward pass\n",
    "        # Return: [batch_size, 1]\n",
    "```\n",
    "\n",
    "**Design Decisions**:\n",
    "- `embedding_dim`: Start with 100 or 200\n",
    "- `hidden_dim`: Try 128 or 256\n",
    "- Pooling: Mean pooling is a good start (handles variable lengths well)\n",
    "- Dropout: 0.3-0.5 to prevent overfitting\n",
    "\n",
    "**Hint**: The forward pass should:\n",
    "1. Apply embedding\n",
    "2. Pool across sequence length dimension\n",
    "3. Pass through hidden layer(s) with activation\n",
    "4. Apply dropout\n",
    "5. Output layer for final prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d1eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Your code here\n",
    "# Create your model class\n",
    "\n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # TODO: Define your layers\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embedding_dim, \n",
    "            padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # x shape: [batch_size, seq_length]        \n",
    "        # Step 1: Embedding\n",
    "        x = self.embedding(x)\n",
    "        # Step 2: Pooling (mean, max, or sum)\n",
    "        pooled = x.mean(dim=1)\n",
    "        # Step 3: Hidden layer(s) with activation\n",
    "        x = self.fc1(pooled)\n",
    "        x = self.relu(x)\n",
    "        # Step 4: Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Step 5: Output layer\n",
    "        output = self.fc2(x)\n",
    "        return output  # shape: [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc962966",
   "metadata": {},
   "source": [
    "## TODO 4: Instantiate and Inspect Your Model üîç\n",
    "\n",
    "**Goal**: Create an instance of your model and understand its structure.\n",
    "\n",
    "**Tasks**:\n",
    "1. **Instantiate the model** with your chosen hyperparameters\n",
    "2. **Move model to device** (CPU or GPU)\n",
    "3. **Print model architecture** using `print(model)`\n",
    "4. **Count parameters** to understand model size\n",
    "5. **Test forward pass** with a dummy batch\n",
    "\n",
    "**Counting Parameters**:\n",
    "```python\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "\n",
    "**Expected hyperparameters**:\n",
    "- `vocab_size`: Your actual vocabulary size (from TODO 1)\n",
    "- `embedding_dim`: 100-300 (start with 100 or 200)\n",
    "- `hidden_dim`: 128-512 (start with 128 or 256)\n",
    "- `padding_idx`: 0 (your `<PAD>` token index)\n",
    "\n",
    "**Test with dummy data**:\n",
    "- Create a dummy batch: `torch.randint(0, vocab_size, (32, 50))`\n",
    "- Pass through model\n",
    "- Check output shape: should be `[32, 1]`\n",
    "\n",
    "**Questions to answer**:\n",
    "- How many trainable parameters does your model have?\n",
    "- What's the shape of the embedding layer weight matrix?\n",
    "- Does your model work with different batch sizes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec210260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DisasterTweetClassifier(\n",
      "  (embedding): Embedding(3160, 100, padding_idx=0)\n",
      "  (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Trainable parameters: 329057\n",
      "Output shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: Your code here\n",
    "# Instantiate and inspect your model\n",
    "\n",
    "# Define helper function\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "vocab_size = len(vocab_dict)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "# Instantiate model\n",
    "model = DisasterTweetClassifier(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Move to device\n",
    "model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Count and print parameters\n",
    "print(f\"Trainable parameters: {count_parameters(model)}\")\n",
    "\n",
    "# Test forward pass with dummy data\n",
    "dummy_data = torch.randint(0, vocab_size, (32, 50))\n",
    "outputs = model(dummy_data)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06aac7",
   "metadata": {},
   "source": [
    "## TODO 5: Understand Loss Function and Optimizer ‚öôÔ∏è\n",
    "\n",
    "**Goal**: Choose and configure loss function and optimizer for binary classification.\n",
    "\n",
    "### Loss Function: Binary Cross-Entropy\n",
    "\n",
    "For binary classification, we use **Binary Cross-Entropy (BCE) Loss**:\n",
    "\n",
    "```python\n",
    "# Two options:\n",
    "# 1. BCELoss - requires sigmoid in model output\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 2. BCEWithLogitsLoss - includes sigmoid (more numerically stable) ‚úÖ RECOMMENDED\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "```\n",
    "\n",
    "**Why BCEWithLogitsLoss?**\n",
    "- More numerically stable (combines sigmoid + BCE)\n",
    "- Less prone to gradient issues\n",
    "- Standard choice for binary classification\n",
    "\n",
    "### Optimizer: Adam\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) is a great default choice:\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "**Learning Rate Guidelines**:\n",
    "- Start with `lr=0.001` (standard default)\n",
    "- Too high ‚Üí unstable training, loss explodes\n",
    "- Too low ‚Üí very slow training\n",
    "- You can adjust later based on training behavior\n",
    "\n",
    "### Optional: Learning Rate Scheduler\n",
    "\n",
    "Reduce learning rate as training progresses:\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "```\n",
    "\n",
    "**Task**: Set up your loss function and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b313cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training Configuration\n",
      "==================================================\n",
      "Loss function: BCEWithLogitsLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x33e2ebb90>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO 5: Your code here\n",
    "# Set up loss function and optimizer\n",
    "\n",
    "# Define loss function\n",
    "from sched import scheduler\n",
    "\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Optional: Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Print configuration\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training Configuration\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {scheduler}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caa00c",
   "metadata": {},
   "source": [
    "## TODO 6: Test Model with Real Data üß™\n",
    "\n",
    "**Goal**: Verify your model works with actual data from your DataLoader.\n",
    "\n",
    "**Tasks**:\n",
    "1. Get one batch from your `train_loader`\n",
    "2. Pass batch through model\n",
    "3. Calculate loss\n",
    "4. Check all shapes are correct\n",
    "5. Verify gradients can be computed\n",
    "\n",
    "**Expected Flow**:\n",
    "```python\n",
    "# Get batch\n",
    "texts, labels = next(iter(train_loader))\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(texts)\n",
    "\n",
    "# Calculate loss\n",
    "loss = loss_fn(outputs, labels)\n",
    "\n",
    "# Backward pass (just to test)\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "**Shapes to verify**:\n",
    "- `texts`: `[batch_size, seq_length]` (e.g., `[32, 50]`)\n",
    "- `labels`: `[batch_size]` (e.g., `[32]`)\n",
    "- `outputs`: `[batch_size, 1]` (e.g., `[32, 1]`)\n",
    "- `loss`: scalar value\n",
    "\n",
    "**Important**: If using `BCEWithLogitsLoss`, you need to:\n",
    "- Ensure labels are float: `labels.float()`\n",
    "- Reshape output if needed: `outputs.squeeze()` or `labels.unsqueeze(1)`\n",
    "\n",
    "**What to check**:\n",
    "- ‚úÖ No shape errors\n",
    "- ‚úÖ Loss is a reasonable number (not NaN, not infinity)\n",
    "- ‚úÖ Gradients are computed\n",
    "- ‚úÖ Model parameters require gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ccd7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 50])\n",
      "Labels shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([32, 1])\n",
      "Number of gradients: 5\n",
      "First gradient sample: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 1.1454e-05, -8.8759e-05, -1.2597e-04,  ...,  5.3586e-05,\n",
      "         -8.7767e-05, -1.8271e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# TODO 6: Your code here\n",
    "# Test model with real data\n",
    "\n",
    "# Get one batch\n",
    "texts, labels = next(iter(train_loader))\n",
    "\n",
    "# Move to device if needed\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(texts)\n",
    "\n",
    "# Calculate loss\n",
    "loss = loss_fn(outputs.squeeze(), labels.float())\n",
    "\n",
    "# Print shapes and values\n",
    "print(f\"Input shape: {texts.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Outputs shape: {outputs.shape}\")\n",
    "texts = texts.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "\n",
    "# Test backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "gradients = [param.grad for param in model.parameters() if param.grad is not None]\n",
    "print(f\"Number of gradients: {len(gradients)}\")\n",
    "print(f\"First gradient sample: {gradients[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed27ba3",
   "metadata": {},
   "source": [
    "## TODO 7: Save Your Model Architecture üíæ\n",
    "\n",
    "**Goal**: Save your model architecture and configuration for the next notebook.\n",
    "\n",
    "You'll need these for training:\n",
    "- Model class definition\n",
    "- Hyperparameters (vocab_size, embedding_dim, hidden_dim)\n",
    "- Model instance\n",
    "- Loss function\n",
    "- Optimizer\n",
    "\n",
    "**Options for saving**:\n",
    "\n",
    "### Option 1: Save just the class definition\n",
    "Create a `src/models/baseline_model.py` file with your model class\n",
    "\n",
    "### Option 2: Save model state dict\n",
    "```python\n",
    "torch.save(model.state_dict(), 'models/baseline_model.pth')\n",
    "```\n",
    "\n",
    "### Option 3: Save entire model\n",
    "```python\n",
    "torch.save(model, 'models/baseline_model_full.pth')\n",
    "```\n",
    "\n",
    "**Recommended approach** for this learning project:\n",
    "- Copy your model class to `src/models/baseline_model.py`\n",
    "- Save hyperparameters in a config dictionary\n",
    "- This way you can import it in the next notebook for training\n",
    "\n",
    "**Task**: \n",
    "1. Create the model file in `src/models/`\n",
    "2. Save your hyperparameters\n",
    "3. Test that you can reload everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e05c068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Your code here\n",
    "# Save model architecture and configuration\n",
    "\n",
    "# Create config dictionary\n",
    "from pandas._config import config\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"padding_idx\": 0\n",
    "}\n",
    "\n",
    "# Option 1: Write model class to file (recommended for learning)\n",
    "# You can manually create src/models/baseline_model.py and copy your class there\n",
    "\n",
    "# Option 2: Save model state\n",
    "# Uncomment if you want to save the initialized model\n",
    "\n",
    "\n",
    "# Test reload\n",
    "from src.models.baseline_model import DisasterTweetClassifier\n",
    "loaded_model = DisasterTweetClassifier(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896b370",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Built your first PyTorch NLP model from scratch\n",
    "- ‚úÖ Understood word embeddings and why they're used\n",
    "- ‚úÖ Designed a neural network architecture for text classification\n",
    "- ‚úÖ Set up loss function and optimizer\n",
    "- ‚úÖ Tested your model with real data\n",
    "- ‚úÖ Prepared everything for training\n",
    "\n",
    "## üìä Model Summary\n",
    "\n",
    "Review what you've created:\n",
    "- **Architecture**: Embedding ‚Üí Pooling ‚Üí Hidden Layer(s) ‚Üí Output\n",
    "- **Parameters**: ~XXX,XXX trainable parameters\n",
    "- **Input**: Word indices `[batch_size, seq_length]`\n",
    "- **Output**: Binary predictions `[batch_size, 1]`\n",
    "- **Loss**: Binary Cross-Entropy\n",
    "- **Optimizer**: Adam\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Your model is ready! Move on to:\n",
    "- **`04_training_and_eval.ipynb`**: Train your model and evaluate performance\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Learnings\n",
    "\n",
    "**What you learned**:\n",
    "1. **Embeddings convert discrete words to continuous vectors** - more efficient than one-hot\n",
    "2. **PyTorch models are classes** inheriting from `nn.Module`\n",
    "3. **Pooling aggregates variable-length sequences** into fixed-size representations\n",
    "4. **BCEWithLogitsLoss is the standard** for binary classification\n",
    "5. **Adam optimizer is a great default** for most deep learning tasks\n",
    "\n",
    "**Questions to reflect on**:\n",
    "- Why do we need pooling? (Variable length ‚Üí Fixed length for FC layers)\n",
    "- What's the difference between BCE and BCEWithLogitsLoss? (Numerical stability)\n",
    "- How many parameters does your embedding layer have? (vocab_size √ó embedding_dim)\n",
    "- Could you add more hidden layers? (Yes! Deeper networks learn more complex patterns)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
