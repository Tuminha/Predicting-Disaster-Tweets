{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 00 - Data Exploration and EDA\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the disaster tweets dataset structure\n",
        "- Explore text characteristics and patterns\n",
        "- Identify data quality issues and preprocessing needs\n",
        "- Build intuition about the classification task\n",
        "\n",
        "## Phase 1: PyTorch Fundamentals ðŸ§ \n",
        "*Build everything from scratch to understand the foundations*\n",
        "\n",
        "## Phase 2: Transformers Enhancement ðŸš€\n",
        "*Enhance with modern NLP tools after mastering fundamentals*\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "**Competition**: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started)\n",
        "\n",
        "**Task**: Binary classification - predict if a tweet is about a real disaster (1) or not (0)\n",
        "\n",
        "**Files**:\n",
        "- `train.csv` - Training data with labels\n",
        "- `test.csv` - Test data for submission (no labels)\n",
        "\n",
        "**Key Columns**:\n",
        "- `id` - Unique identifier\n",
        "- `text` - Tweet content\n",
        "- `target` - Label (1=disaster, 0=not disaster)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 1: Load and Inspect Data\n",
        "\n",
        "**Goal**: Load the dataset and perform basic inspection\n",
        "\n",
        "**Steps**:\n",
        "1. Import necessary libraries (pandas, numpy, matplotlib, seaborn)\n",
        "2. Load `train.csv` and `test.csv` from `data/raw/`\n",
        "3. Display basic dataset information:\n",
        "   - Shape (number of rows and columns)\n",
        "   - Column names and data types\n",
        "   - First few rows\n",
        "   - Memory usage\n",
        "\n",
        "**Hint**: Use `pd.read_csv()`, `df.info()`, `df.head()`, and `df.memory_usage()`\n",
        "\n",
        "**Expected Output**: Understanding of dataset size and structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 1: Load and inspect data\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 2: Target Distribution Analysis\n",
        "\n",
        "**Goal**: Understand the class distribution and balance\n",
        "\n",
        "**Steps**:\n",
        "1. Analyze target distribution:\n",
        "   - Count of each class (0 vs 1)\n",
        "   - Percentage distribution\n",
        "   - Visualize with bar chart\n",
        "2. Check for any missing values in target column\n",
        "3. Analyze class balance implications for modeling\n",
        "\n",
        "**Hint**: Use `df['target'].value_counts()`, `df['target'].value_counts(normalize=True)`, and `sns.countplot()`\n",
        "\n",
        "**Expected Output**: Understanding of whether the dataset is balanced and potential impact on model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 2: Target distribution analysis\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 3: Text Length Analysis\n",
        "\n",
        "**Goal**: Understand text characteristics and length patterns\n",
        "\n",
        "**Steps**:\n",
        "1. Calculate text length statistics:\n",
        "   - Character count per tweet\n",
        "   - Word count per tweet\n",
        "   - Sentence count per tweet\n",
        "2. Visualize length distributions:\n",
        "   - Histograms for character and word counts\n",
        "   - Box plots by target class\n",
        "3. Identify outliers and potential preprocessing needs\n",
        "\n",
        "**Hint**: Use `df['text'].str.len()`, `df['text'].str.split().str.len()`, and matplotlib/seaborn for visualization\n",
        "\n",
        "**Expected Output**: Understanding of text length patterns and preprocessing requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 3: Text length analysis\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 4: Word Frequency Analysis\n",
        "\n",
        "**Goal**: Identify common words and patterns in disaster vs non-disaster tweets\n",
        "\n",
        "**Steps**:\n",
        "1. Create word frequency analysis:\n",
        "   - Most common words overall\n",
        "   - Most common words by class (disaster vs non-disaster)\n",
        "   - Word clouds for visual representation\n",
        "2. Analyze differences between classes:\n",
        "   - Words unique to disaster tweets\n",
        "   - Words unique to non-disaster tweets\n",
        "   - Statistical significance of word differences\n",
        "\n",
        "**Hint**: Use `Counter`, `collections`, and `wordcloud` library for visualization\n",
        "\n",
        "**Expected Output**: Understanding of vocabulary patterns that might help classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 4: Word frequency analysis\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 5: Data Quality Assessment\n",
        "\n",
        "**Goal**: Identify data quality issues and preprocessing requirements\n",
        "\n",
        "**Steps**:\n",
        "1. Check for missing values in all columns\n",
        "2. Identify duplicate tweets\n",
        "3. Analyze special characters and URLs:\n",
        "   - Count of URLs per tweet\n",
        "   - Count of hashtags, mentions, emojis\n",
        "   - Special character patterns\n",
        "4. Sample and manually review examples from each class\n",
        "\n",
        "**Hint**: Use `df.isnull().sum()`, `df.duplicated().sum()`, and regex patterns for special characters\n",
        "\n",
        "**Expected Output**: List of data quality issues and preprocessing steps needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO 5: Data quality assessment\n",
        "# Your implementation here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 6: Key Insights and Next Steps\n",
        "\n",
        "**Goal**: Synthesize findings and plan preprocessing strategy\n",
        "\n",
        "**Steps**:\n",
        "1. Summarize key findings from exploration:\n",
        "   - Dataset characteristics\n",
        "   - Class distribution insights\n",
        "   - Text length patterns\n",
        "   - Vocabulary differences\n",
        "   - Data quality issues\n",
        "2. Document preprocessing requirements:\n",
        "   - Text cleaning steps needed\n",
        "   - Tokenization strategy\n",
        "   - Vocabulary size considerations\n",
        "   - Sequence length decisions\n",
        "3. Plan train/validation/test split strategy\n",
        "\n",
        "**Expected Output**: Clear roadmap for preprocessing phase\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Transformers Enhancement\n",
        "\n",
        "*After completing Phase 1, consider these enhancements:*\n",
        "\n",
        "- Use HuggingFace datasets for efficient data loading\n",
        "- Leverage pre-trained tokenizers (BERT, RoBERTa)\n",
        "- Compare custom preprocessing vs. transformer tokenization\n",
        "- Analyze how transformer tokenization handles special characters differently\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
