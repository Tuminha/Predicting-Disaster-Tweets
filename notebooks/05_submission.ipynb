{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f87f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "SETUP COMPLETE - Ready for predictions!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO 1: Setup and Import Everything\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"‚úÖ Added to path: {project_root}\")\n",
    "\n",
    "# Import your model\n",
    "from src.models.baseline_model import DisasterTweetClassifier\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SETUP COMPLETE - Ready for predictions!\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1681218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary...\n",
      "‚úÖ Vocabulary loaded: 14646 words\n",
      "\n",
      "Loading test data...\n",
      "‚úÖ Test data loaded: 3263 samples\n",
      "Columns: ['id', 'keyword', 'location', 'text', 'has_hashtag', 'has_mention', 'has_url', 'number_urls', 'number_hashtags', 'number_mentions', 'word_count', 'text_clean', 'emojis_text', 'word_count_difference']\n",
      "\n",
      "First few rows:\n",
      "   id keyword location                                               text  \\\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
      "\n",
      "   has_hashtag  has_mention  has_url  number_urls  number_hashtags  \\\n",
      "0        False        False    False            0                0   \n",
      "1         True        False    False            0                1   \n",
      "2        False        False    False            0                0   \n",
      "3         True        False    False            0                2   \n",
      "4        False        False    False            0                0   \n",
      "\n",
      "   number_mentions  word_count  \\\n",
      "0                0           6   \n",
      "1                0           9   \n",
      "2                0          19   \n",
      "3                0           4   \n",
      "4                0           8   \n",
      "\n",
      "                                          text_clean emojis_text  \\\n",
      "0                        happened terrible car crash         NaN   \n",
      "1  heard earthquake different cities stay safe ev...         NaN   \n",
      "2  forest fire spot pond geese fleeing across str...         NaN   \n",
      "3              apocalypse lighting spokane wildfires         NaN   \n",
      "4                typhoon soudelor kills china taiwan         NaN   \n",
      "\n",
      "   word_count_difference  \n",
      "0                      2  \n",
      "1                      2  \n",
      "2                      9  \n",
      "3                      0  \n",
      "4                      3  \n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Load Vocabulary and Test Data\n",
    "\n",
    "print(\"Loading vocabulary...\")\n",
    "vocab_dict = pickle.load(open('../data/processed/vocab_dict.pkl', 'rb'))\n",
    "vocab_size = len(vocab_dict)\n",
    "print(f\"‚úÖ Vocabulary loaded: {vocab_size} words\")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_df = pd.read_csv(\"../data/interim/test_cleaned.csv\")\n",
    "print(f\"‚úÖ Test data loaded: {len(test_df)} samples\")\n",
    "print(f\"Columns: {test_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c408c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined (identical to training)\n",
      "‚úÖ Ready to create test DataLoader\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Recreate Helper Functions (Same as Training!)\n",
    "\n",
    "# Text-to-sequence function (MUST be identical to training!)\n",
    "def text_to_sequence(text, vocab_dict, max_length=50, pad_idx=0, unk_idx=1):\n",
    "    \"\"\"Convert text to sequence of indices - SAME AS TRAINING!\"\"\"\n",
    "    words = text.lower().split()\n",
    "    sequence = [vocab_dict.get(word, unk_idx) for word in words]\n",
    "    \n",
    "    if len(sequence) < max_length:\n",
    "        sequence.extend([pad_idx] * (max_length - len(sequence)))\n",
    "    else:\n",
    "        sequence = sequence[:max_length]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Dataset class (SAME AS TRAINING!)\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, texts, vocab_dict, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        return text\n",
    "\n",
    "# Collate function (SAME AS TRAINING!)\n",
    "def collate_fn(batch, vocab_dict, max_length):\n",
    "    sequences = [text_to_sequence(text, vocab_dict, max_length) for text in batch]\n",
    "    sequences = torch.tensor(sequences)\n",
    "    return sequences\n",
    "\n",
    "print(\"‚úÖ Helper functions defined (identical to training)\")\n",
    "print(\"‚úÖ Ready to create test DataLoader\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f86a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test DataLoader...\n",
      "‚úÖ Test DataLoader created\n",
      "Number of batches: 102\n",
      "Batch size: 32\n",
      "Total samples: 3263\n",
      "‚úÖ Batch shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: Create Test DataLoader\n",
    "\n",
    "print(\"Creating test DataLoader...\")\n",
    "\n",
    "# Create DataLoader (no labels for test data!)\n",
    "test_loader = DataLoader(\n",
    "    DisasterTweetsDataset(test_df[\"text_clean\"], vocab_dict, max_length=50),\n",
    "    batch_size=32,\n",
    "    shuffle=False,  # IMPORTANT: Don't shuffle test data!\n",
    "    collate_fn=lambda batch: collate_fn(batch, vocab_dict, max_length=50)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test DataLoader created\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Total samples: {len(test_df)}\")\n",
    "\n",
    "# Test the loader\n",
    "for batch in test_loader:\n",
    "    print(f\"‚úÖ Batch shape: {batch.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd16243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "‚úÖ Model loaded from: ../src/models/disaster_tweet_classifier.pth\n",
      "‚úÖ Model set to evaluation mode\n",
      "Model architecture:\n",
      "DisasterTweetClassifier(\n",
      "  (embedding): Embedding(14646, 50, padding_idx=0)\n",
      "  (fc1): Linear(in_features=50, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.6, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TODO 5: Load Trained Model\n",
    "\n",
    "print(\"Loading trained model...\")\n",
    "\n",
    "# Model hyperparameters (MUST match training!)\n",
    "embedding_dim = 50   # Same as training\n",
    "hidden_dim = 64      # Same as training\n",
    "\n",
    "# Initialize model with same architecture\n",
    "model = DisasterTweetClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    padding_idx=0\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "model_path = '../src/models/disaster_tweet_classifier.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# IMPORTANT: Set to evaluation mode!\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded from: {model_path}\")\n",
    "print(f\"‚úÖ Model set to evaluation mode\")\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5673b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "This may take a minute...\n",
      "Processed 640 / 3263 samples...\n",
      "Processed 1280 / 3263 samples...\n",
      "Processed 1920 / 3263 samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2560 / 3263 samples...\n",
      "Processed 3200 / 3263 samples...\n",
      "\n",
      "‚úÖ Predictions complete!\n",
      "Total predictions: 3263\n",
      "Predictions distribution:\n",
      "  - Disaster (1): 1275\n",
      "  - Not Disaster (0): 1988\n",
      "  - Disaster ratio: 39.07%\n"
     ]
    }
   ],
   "source": [
    "# TODO 6: Generate Predictions on Test Set\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "# Storage for predictions\n",
    "all_predictions = []\n",
    "\n",
    "# No gradient computation needed for inference\n",
    "with torch.no_grad():\n",
    "    for batch_idx, texts in enumerate(test_loader):\n",
    "        # Move batch to device\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        # Convert logits to probabilities, then to binary predictions\n",
    "        # sigmoid(output) > 0.5 ‚Üí class 1 (disaster), else ‚Üí class 0\n",
    "        probabilities = torch.sigmoid(outputs.squeeze())\n",
    "        predictions = (probabilities > 0.5).int()\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        \n",
    "        # Print progress every 20 batches\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            print(f\"Processed {(batch_idx + 1) * 32} / {len(test_df)} samples...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions complete!\")\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "print(f\"Predictions distribution:\")\n",
    "print(f\"  - Disaster (1): {sum(all_predictions)}\")\n",
    "print(f\"  - Not Disaster (0): {len(all_predictions) - sum(all_predictions)}\")\n",
    "print(f\"  - Disaster ratio: {sum(all_predictions) / len(all_predictions):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f7f6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "‚úÖ Submission DataFrame created\n",
      "\n",
      "Submission shape: (3263, 2)\n",
      "Columns: ['id', 'target']\n",
      "\n",
      "First few rows:\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n",
      "5  12       1\n",
      "6  21       0\n",
      "7  22       0\n",
      "8  27       0\n",
      "9  29       0\n",
      "\n",
      "Last few rows:\n",
      "         id  target\n",
      "3253  10838       1\n",
      "3254  10845       1\n",
      "3255  10856       1\n",
      "3256  10857       0\n",
      "3257  10858       1\n",
      "3258  10861       0\n",
      "3259  10865       1\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       1\n"
     ]
    }
   ],
   "source": [
    "# TODO 7: Create Submission DataFrame\n",
    "\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "# Load original test data to get the 'id' column\n",
    "original_test = pd.read_csv(\"../data/raw/test.csv\")\n",
    "\n",
    "# Create submission DataFrame with required format\n",
    "submission = pd.DataFrame({\n",
    "    'id': original_test['id'],\n",
    "    'target': all_predictions\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Submission DataFrame created\")\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Columns: {submission.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(submission.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c3d58f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating submission format...\n",
      "==================================================\n",
      "‚úÖ Columns correct: ['id', 'target']\n",
      "‚úÖ Row count correct: 3263\n",
      "‚úÖ No missing values\n",
      "‚úÖ Target values correct: [0, 1]\n",
      "‚úÖ ID column matches original test data\n",
      "==================================================\n",
      "‚úÖ ALL VALIDATION CHECKS PASSED!\n",
      "Ready to save and submit to Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# TODO 8: Validate Submission Format\n",
    "\n",
    "print(\"Validating submission format...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check 1: Correct columns\n",
    "expected_columns = ['id', 'target']\n",
    "if list(submission.columns) == expected_columns:\n",
    "    print(\"‚úÖ Columns correct: ['id', 'target']\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Expected {expected_columns}, got {list(submission.columns)}\")\n",
    "\n",
    "# Check 2: Correct number of rows\n",
    "expected_rows = len(original_test)\n",
    "if len(submission) == expected_rows:\n",
    "    print(f\"‚úÖ Row count correct: {expected_rows}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Expected {expected_rows} rows, got {len(submission)}\")\n",
    "\n",
    "# Check 3: No missing values\n",
    "if submission.isnull().sum().sum() == 0:\n",
    "    print(\"‚úÖ No missing values\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Found missing values:\\n{submission.isnull().sum()}\")\n",
    "\n",
    "# Check 4: Target values are 0 or 1\n",
    "unique_targets = submission['target'].unique()\n",
    "if set(unique_targets).issubset({0, 1}):\n",
    "    print(f\"‚úÖ Target values correct: {sorted(unique_targets)}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: Target should be 0 or 1, found: {unique_targets}\")\n",
    "\n",
    "# Check 5: ID column matches original\n",
    "if submission['id'].equals(original_test['id']):\n",
    "    print(\"‚úÖ ID column matches original test data\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: ID column mismatch\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ ALL VALIDATION CHECKS PASSED!\")\n",
    "print(\"Ready to save and submit to Kaggle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6593ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission saved to: ../data/submissions/submission.csv\n",
      "\n",
      "File size: 22.21 KB\n",
      "\n",
      "==================================================\n",
      "üéâ SUBMISSION FILE READY!\n",
      "='*50}\n",
      "\n",
      "Next steps:\n",
      "1. Go to: https://www.kaggle.com/competitions/nlp-getting-started/submit\n",
      "2. Click 'Submit Predictions'\n",
      "3. Upload: submissions/submission.csv\n",
      "4. Wait for your score!\n",
      "\n",
      "üìä Expected F1-Score: ~0.76-0.78 (based on validation)\n",
      "üéØ Your validation accuracy was 80%, so expect similar on test set!\n"
     ]
    }
   ],
   "source": [
    "# TODO 9: Save Submission File\n",
    "\n",
    "# Create submissions directory if it doesn't exist\n",
    "os.makedirs('../data/submissions', exist_ok=True)\n",
    "\n",
    "# Save submission file\n",
    "submission_path = '../data/submissions/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Submission saved to: {submission_path}\")\n",
    "print(f\"\\nFile size: {os.path.getsize(submission_path) / 1024:.2f} KB\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üéâ SUBMISSION FILE READY!\")\n",
    "print(\"='*50}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Go to: https://www.kaggle.com/competitions/nlp-getting-started/submit\")\n",
    "print(\"2. Click 'Submit Predictions'\")\n",
    "print(\"3. Upload: submissions/submission.csv\")\n",
    "print(\"4. Wait for your score!\")\n",
    "print(\"\\nüìä Expected F1-Score: ~0.76-0.78 (based on validation)\")\n",
    "print(\"üéØ Your validation accuracy was 80%, so expect similar on test set!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fda3f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions to review:\n",
      "================================================================================\n",
      "\n",
      "ID: 4390 | Prediction: üö® DISASTER\n",
      "Original: 1.9 magnitude #earthquake. 16 km from Anchorage AK #UnitedStates http://t.co/wVu08yJGOK...\n",
      "Cleaned:  magnitude earthquake anchorage unitedstates...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 9968 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: TheDIYHacks: RT Illusionimagess: A tsunami cloud. http://t.co/H6t4x1EVTx...\n",
      "Cleaned:  thediyhacks tsunami cloud...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 6205 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: Remove http://t.co/GYcBuXfYGP Browser Hijack... http://t.co/FuQH0yKsdg...\n",
      "Cleaned:  remove browser hijack...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 10196 | Prediction: üö® DISASTER\n",
      "Original: Violent Forces Radio: Now Playing Axegressor - Psalm Before the Storm\n",
      "TuneIn Player @ http://t.co/Xs...\n",
      "Cleaned:  violent forces radio playing axegressor psalm storm tunein player...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 8811 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: pos nose que hago escuchando Sleeping With Sirens - Iris (Goo Goo Dolls Cover) KE PEDO ._....\n",
      "Cleaned:  pos nose que hago escuchando sleeping sirens iris goo goo dolls cover pedo...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 7485 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: @KDonhoops Sure! Meek Mill accused Drake of not writing his own raps. Drake responds w/ obliteration...\n",
      "Cleaned:  sure meek mill accused drake writing raps drake responds obliteration meeks career...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 3809 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: Apollo Brown ¬â√õ√í Detonate feat. M.O.P.: Producer Apollo Brown is proud to present his upcoming album...\n",
      "Cleaned:  apollo brown ¬â√ª√≤ detonate feat mop producer apollo brown proud present upcoming album grandeur...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 7524 | Prediction: üö® DISASTER\n",
      "Original: When the Government couldn't figure out how to plug the Gulf oil spill so Chuck Norris plugged it......\n",
      "Cleaned:  government couldnt figure plug gulf oil spill chuck norris plugged itwith fist...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 6508 | Prediction: ‚úÖ NOT DISASTER\n",
      "Original: cmon injury bug this shit aint funny we suffered enough injuries last year......\n",
      "Cleaned:  cmon injury bug shit aint funny suffered enough injuries last year...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID: 8014 | Prediction: üö® DISASTER\n",
      "Original: 12000 Nigerian refugees repatriated from Cameroon http://t.co/LeLYa0vDOg read /////...\n",
      "Cleaned:  12000 nigerian refugees repatriated cameroon read...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Analyze Some Predictions\n",
    "\n",
    "print(\"Sample predictions to review:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show 10 random samples with their predictions\n",
    "sample_indices = np.random.choice(len(submission), 10, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    tweet_id = submission.iloc[idx]['id']\n",
    "    prediction = submission.iloc[idx]['target']\n",
    "    original_text = original_test[original_test['id'] == tweet_id]['text'].values[0]\n",
    "    cleaned_text = test_df.iloc[idx]['text_clean']\n",
    "    \n",
    "    pred_label = \"üö® DISASTER\" if prediction == 1 else \"‚úÖ NOT DISASTER\"\n",
    "    \n",
    "    print(f\"\\nID: {tweet_id} | Prediction: {pred_label}\")\n",
    "    print(f\"Original: {original_text[:100]}...\")\n",
    "    print(f\"Cleaned:  {cleaned_text[:100]}...\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed772239",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the full machine learning pipeline:\n",
    "\n",
    "### ‚úÖ What You Accomplished:\n",
    "1. **Data Exploration** - Understood your dataset deeply\n",
    "2. **Preprocessing** - Built a robust text cleaning pipeline\n",
    "3. **Feature Engineering** - Created vocabulary and data loaders\n",
    "4. **Model Architecture** - Designed a custom PyTorch classifier\n",
    "5. **Training** - Achieved 80% validation accuracy through systematic tuning\n",
    "6. **Evaluation** - Comprehensive metrics analysis\n",
    "7. **Submission** - Generated predictions for Kaggle!\n",
    "\n",
    "### üìä Your Model Performance:\n",
    "- **Validation Accuracy**: 80%\n",
    "- **F1-Score**: 0.76\n",
    "- **Precision**: 78%\n",
    "- **Recall**: 74%\n",
    "\n",
    "### üéØ What to Expect on Kaggle:\n",
    "- Your test score should be close to your validation score (~76-78% F1)\n",
    "- If it's significantly different, there might be:\n",
    "  - Different data distribution\n",
    "  - Overfitting (if much worse)\n",
    "  - Lucky predictions (if much better)\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Submit and get your baseline score**\n",
    "2. **Analyze the leaderboard position**\n",
    "3. **Decide on improvements:**\n",
    "   - Try different architectures (LSTM, GRU)\n",
    "   - Experiment with pre-trained embeddings (GloVe, Word2Vec)\n",
    "   - Move to Phase 2: **Transformers** (BERT, RoBERTa) for potential +5-10% boost!\n",
    "\n",
    "### üìö Learning Achievements:\n",
    "You now understand:\n",
    "- End-to-end ML pipeline\n",
    "- Text preprocessing for NLP\n",
    "- PyTorch model development\n",
    "- Training loop implementation\n",
    "- Hyperparameter tuning\n",
    "- Overfitting diagnosis and fixes\n",
    "- Model evaluation and metrics\n",
    "\n",
    "**This is professional-level ML engineering work!** üéìüí™\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Reflection Questions:\n",
    "1. How did your model perform compared to your expectations?\n",
    "2. What was the hardest part of this project?\n",
    "3. What would you try differently next time?\n",
    "4. Are you ready to try Transformers and beat your own score?\n",
    "\n",
    "**Good luck with your submission!** üçÄüöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0618e",
   "metadata": {},
   "source": [
    "# üöÄ Kaggle Submission - Make Predictions on Test Set\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "Time to make predictions and submit to Kaggle! You'll:\n",
    "- Load your trained model\n",
    "- Preprocess test data\n",
    "- Generate predictions\n",
    "- Create submission file\n",
    "- Submit to Kaggle competition\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "1. **Load trained model** from saved state\n",
    "2. **Apply preprocessing** to new, unseen test data\n",
    "3. **Generate predictions** in batch mode\n",
    "4. **Create Kaggle submission** format\n",
    "5. **Submit and get feedback** from leaderboard\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 1: Load Trained Model and Test Data\n",
    "\n",
    "- Load your best model from `../models/disaster_classifier.pth`\n",
    "- Load test data from `../data/raw/test.csv`\n",
    "- Load your vocabulary (vocab_dict)\n",
    "- Set model to eval mode\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 2: Preprocess Test Data\n",
    "\n",
    "Apply the SAME preprocessing as training data:\n",
    "- Use functions from `01_preprocessing.ipynb`\n",
    "- Convert text to sequences using your vocabulary\n",
    "- Handle unknown words with `<UNK>` token\n",
    "- Pad/truncate to same max_length (50)\n",
    "- Create DataLoader (no shuffle needed)\n",
    "\n",
    "**Critical**: Preprocessing must match training exactly!\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 3: Generate Predictions\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts in test_loader:\n",
    "        outputs = model(texts)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 4: Create Submission File\n",
    "\n",
    "Kaggle expects this format:\n",
    "```\n",
    "id,target\n",
    "0,1\n",
    "2,1\n",
    "3,1\n",
    "9,0\n",
    "...\n",
    "```\n",
    "\n",
    "```python\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('../submissions/submission.csv', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 5: Validate Submission Format\n",
    "\n",
    "Check before submitting:\n",
    "- Correct number of rows (3,263 for this competition)\n",
    "- Columns are 'id' and 'target'\n",
    "- No missing values\n",
    "- target is 0 or 1 only\n",
    "\n",
    "---\n",
    "\n",
    "## TODO 6: Submit to Kaggle üéâ\n",
    "\n",
    "1. Go to competition page\n",
    "2. Click \"Submit Predictions\"\n",
    "3. Upload your `submission.csv`\n",
    "4. Wait for score!\n",
    "\n",
    "**Competition metric**: F1-Score\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Next Steps\n",
    "\n",
    "After your first submission:\n",
    "- Analyze which tweets were misclassified\n",
    "- Try different architectures (LSTM, GRU)\n",
    "- Experiment with hyperparameters\n",
    "- Move to Phase 2: HuggingFace Transformers!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
