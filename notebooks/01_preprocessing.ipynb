{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Text Preprocessing and Cleaning\n",
        "\n",
        "## Learning Objectives\n",
        "- Master text preprocessing techniques for NLP\n",
        "- Understand the impact of different cleaning strategies\n",
        "- Build a robust preprocessing pipeline\n",
        "- Prepare data for tokenization and modeling\n",
        "\n",
        "## Phase 1: PyTorch Fundamentals ðŸ§ \n",
        "*Build everything from scratch to understand the foundations*\n",
        "\n",
        "## Phase 2: Transformers Enhancement ðŸš€\n",
        "*Enhance with modern NLP tools after mastering fundamentals*\n",
        "\n",
        "---\n",
        "\n",
        "## Preprocessing Strategy\n",
        "\n",
        "Based on your exploration findings, implement a comprehensive text cleaning pipeline that handles:\n",
        "- Special characters and URLs\n",
        "- Case normalization\n",
        "- Punctuation handling\n",
        "- Stopword removal (optional)\n",
        "- Custom cleaning rules for social media text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 1: Basic Text Cleaning Functions\n",
        "\n",
        "**Goal**: Create reusable text cleaning functions\n",
        "\n",
        "**Steps**:\n",
        "1. Implement URL removal function:\n",
        "   - Remove http/https URLs\n",
        "   - Handle shortened URLs\n",
        "2. Implement username/hashtag handling:\n",
        "   - Decide whether to remove or keep @mentions and #hashtags\n",
        "3. Implement special character cleaning:\n",
        "   - Handle emojis (remove or convert to text)\n",
        "   - Clean punctuation\n",
        "   - Handle repeated characters (e.g., \"soooooo\" â†’ \"so\")\n",
        "\n",
        "**Hint**: Use `re` module for regex patterns, consider different strategies for each element\n",
        "\n",
        "**Expected Output**: Clean, reusable functions for text preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average number of urls per tweet: 0.6201234730066991\n",
            "Average number of hashtags per tweet: 0.4469985551031131\n",
            "Average number of mentions per tweet: 0.36240641008800734\n",
            "0    our deeds are the reason of this earthquake ma...\n",
            "1               forest fire near la ronge sask. canada\n",
            "2    all residents asked to 'shelter in place' are ...\n",
            "3    13,000 people receive wildfires evacuation ord...\n",
            "4    just got sent this photo from ruby alaska as s...\n",
            "Name: text_clean, dtype: object\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  has_hashtag  has_mention  has_url  number_urls  number_hashtags  \\\n",
            "0       1         True        False    False            0                1   \n",
            "1       1        False        False    False            0                0   \n",
            "2       1        False        False    False            0                0   \n",
            "3       1         True        False    False            0                1   \n",
            "4       1         True        False    False            0                2   \n",
            "\n",
            "   number_mentions                                         text_clean  \\\n",
            "0                0  our deeds are the reason of this earthquake ma...   \n",
            "1                0             forest fire near la ronge sask. canada   \n",
            "2                0  all residents asked to 'shelter in place' are ...   \n",
            "3                0  13,000 people receive wildfires evacuation ord...   \n",
            "4                0  just got sent this photo from ruby alaska as s...   \n",
            "\n",
            "  emojis_text  \n",
            "0        None  \n",
            "1        None  \n",
            "2        None  \n",
            "3        None  \n",
            "4        None  \n"
          ]
        }
      ],
      "source": [
        "# TODO 1: Basic text cleaning functions\n",
        "# Your implementation here\n",
        "# Create url removal function\n",
        "import re\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"../data/raw/train.csv\")\n",
        "\n",
        "def remove_url(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", text)\n",
        "\n",
        "# Define a function to count emojis in a string\n",
        "def count_emojis(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    return sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r\"@[^\\s]+\", \"\", text)\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r\"#\", \"\", text)\n",
        "\n",
        "def convert_emojis_to_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Replace emoji characters in the text with their text names (e.g., \":smile:\")\n",
        "    return emoji.demojize(text, delimiters=(\"\", \"\"))\n",
        "\n",
        "def get_emoji_text_or_none(text):\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    emoji_text = convert_emojis_to_text(text)\n",
        "    # If any emoji exists (detected by presence of \":\" from demojize output)\n",
        "    if ':' in emoji_text:\n",
        "        return emoji_text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Column if hashtag and if mention\n",
        "df[\"has_hashtag\"] = df[\"text\"].str.contains(r\"#\")\n",
        "df[\"has_mention\"] = df[\"text\"].str.contains(r\"@\")\n",
        "df[\"has_url\"] = df[\"text\"].str.contains(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "\n",
        "df[\"number_urls\"] = df[\"text\"].str.count(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "df[\"number_hashtags\"] = df[\"text\"].str.count(r\"#\")\n",
        "df[\"number_mentions\"] = df[\"text\"].str.count(r\"@\")\n",
        "\n",
        "# Average number of urls, hashtags, mentions, and emojis per tweet\n",
        "print(f\"Average number of urls per tweet: {df['number_urls'].mean()}\")\n",
        "print(f\"Average number of hashtags per tweet: {df['number_hashtags'].mean()}\")\n",
        "print(f\"Average number of mentions per tweet: {df['number_mentions'].mean()}\")\n",
        "\n",
        "# Create text_clean column lowercased\n",
        "df[\"text_clean\"] = df[\"text\"].apply(lambda x: remove_url(x).lower())\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_mentions(x).lower())\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(lambda x: remove_hashtags(x).lower())\n",
        "\n",
        "\n",
        "\n",
        "df[\"emojis_text\"] = df[\"text\"].apply(get_emoji_text_or_none)\n",
        "\n",
        "print(df[\"text_clean\"].head())\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 2: Text Normalization\n",
        "\n",
        "**Goal**: Standardize text format for consistent processing\n",
        "\n",
        "**Steps**:\n",
        "1. Implement case normalization:\n",
        "   - Decide on lowercasing strategy\n",
        "   - Handle acronyms and proper nouns\n",
        "2. Implement whitespace normalization:\n",
        "   - Remove extra spaces\n",
        "   - Handle line breaks and tabs\n",
        "3. Implement number handling:\n",
        "   - Replace numbers with tokens or remove\n",
        "   - Handle dates and times\n",
        "\n",
        "**Hint**: Consider the impact on meaning - some information might be lost in normalization\n",
        "\n",
        "**Expected Output**: Consistent text format across the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello one hundred and twenty-three, how are you? four hundred and fifty-six\n"
          ]
        }
      ],
      "source": [
        "# TODO 2: Text normalization\n",
        "# Your implementation here\n",
        "import inflect\n",
        "def whitespace_normalization(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def replace_numbers_with_tokens(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    p = inflect.engine()\n",
        "    def number_to_text(match):\n",
        "        numstr = match.group()\n",
        "        return p.number_to_words(numstr)\n",
        "    return re.sub(r'\\d+', number_to_text, text)\n",
        "\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(whitespace_normalization)\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(replace_numbers_with_tokens)\n",
        "\n",
        "# test\n",
        "test_text = \"Hello 123, how are you? 456\"\n",
        "print(replace_numbers_with_tokens(test_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 3: Stopword and Noise Removal\n",
        "\n",
        "**Goal**: Remove common words and noise that don't contribute to classification\n",
        "\n",
        "**Steps**:\n",
        "1. Implement stopword removal:\n",
        "   - Use NLTK or spaCy stopword lists\n",
        "   - Consider domain-specific stopwords\n",
        "   - Evaluate impact on disaster detection\n",
        "2. Implement noise removal:\n",
        "   - Remove very short words (< 2 characters)\n",
        "   - Remove very long words (potential typos)\n",
        "   - Handle non-alphabetic sequences\n",
        "\n",
        "**Hint**: Be careful with stopwords - some might be important for disaster detection (e.g., \"fire\", \"flood\")\n",
        "\n",
        "**Expected Output**: Cleaned text with reduced noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/franciscoteixeirabarbosa/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head before removing stop words:\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  has_hashtag  has_mention  has_url  number_urls  number_hashtags  \\\n",
            "0       1         True        False    False            0                1   \n",
            "1       1        False        False    False            0                0   \n",
            "2       1        False        False    False            0                0   \n",
            "3       1         True        False    False            0                1   \n",
            "4       1         True        False    False            0                2   \n",
            "\n",
            "   number_mentions                                         text_clean  \\\n",
            "0                0                     deeds reason may allah forgive   \n",
            "1                0                      forest near ronge sask canada   \n",
            "2                0  residents asked shelter place notified officer...   \n",
            "3                0  NUMBERNUMBER people receive wildfires orders c...   \n",
            "4                0  got sent photo ruby alaska smoke wildfires pou...   \n",
            "\n",
            "  emojis_text  word_count  \n",
            "0        None           5  \n",
            "1        None           5  \n",
            "2        None          10  \n",
            "3        None           6  \n",
            "4        None           9  \n",
            "Head after removing stop words and short/long words:\n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  has_hashtag  has_mention  has_url  number_urls  number_hashtags  \\\n",
            "0       1         True        False    False            0                1   \n",
            "1       1        False        False    False            0                0   \n",
            "2       1        False        False    False            0                0   \n",
            "3       1         True        False    False            0                1   \n",
            "4       1         True        False    False            0                2   \n",
            "\n",
            "   number_mentions                                         text_clean  \\\n",
            "0                0                     deeds reason may allah forgive   \n",
            "1                0                      forest near ronge sask canada   \n",
            "2                0  residents asked shelter place notified officer...   \n",
            "3                0  numbernumber people receive wildfires orders c...   \n",
            "4                0  got sent photo ruby alaska smoke wildfires pou...   \n",
            "\n",
            "  emojis_text  word_count  \n",
            "0        None           5  \n",
            "1        None           5  \n",
            "2        None          10  \n",
            "3        None           6  \n",
            "4        None           9  \n",
            "fire forest disaster caused lot damage.\n",
            "The fire the forest disaster that has caused lot damage.\n",
            "The fire in the forest is a disaster that has caused a lot of damage\n"
          ]
        }
      ],
      "source": [
        "# TODO 3: Stopword and noise removal\n",
        "# Your implementation here\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Implement stop word removal but keeping some 50 critical words related to disasters\n",
        "disaster_50_words_list = [\n",
        "    \"fire\", \"flood\", \"earthquake\", \"hurricane\", \"tornado\", \"tsunami\", \"volcano\", \"wildfire\", \"storm\", \"hail\",\n",
        "    \"avalanche\", \"drought\", \"cyclone\", \"disaster\", \"emergency\", \"crisis\", \"accident\", \"collapse\", \"explosion\", \"meteorite\",\n",
        "    \"hunger\", \"disease\", \"death\", \"injury\", \"damage\", \"destruction\", \"evacuation\", \"pandemic\", \"eruption\", \"aftershock\", \"landslide\",\n",
        "    \"mudslide\", \"outbreak\", \"plague\", \"rescue\", \"survivors\", \"fatality\", \"wreckage\", \"contamination\", \"quarantine\", \"hazard\",\n",
        "    \"sinkhole\", \"blackout\", \"blizzard\", \"storm surge\", \"typhoon\", \"structural\", \"apocalypse\", \"casualty\", \"distress\", \"emergency services\"\n",
        "]\n",
        "\n",
        "df[\"word_count\"] = df[\"text_clean\"].apply(lambda x: len(x.split()))\n",
        "print(\"Head before removing stop words:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create a stopwords set but EXCLUDING the disaster-related words so that those are not removed\n",
        "# Handle multi-word phrases: remove them from stopwords if present as phrases or as separated words (normalize to lower-case)\n",
        "stopword_set = set(stopwords.words('english'))\n",
        "\n",
        "# Remove all single-word disaster keywords from the stopword set, NOT from the text\n",
        "for word in disaster_50_words_list:\n",
        "    # Only remove from stopword_set the single words (if phrase, will be handled in text)\n",
        "    for w in word.lower().split():\n",
        "        if w in stopword_set:\n",
        "            stopword_set.remove(w)\n",
        "\n",
        "# Function to remove stop words while keeping disaster-related words\n",
        "def remove_stop_words(text, stop_words=stopword_set, keywords=disaster_50_words_list):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # For multi-word keywords, preserve them by temporarily replacing with placeholders\n",
        "    placeholder_map = {}\n",
        "    text_norm = text\n",
        "    for idx, phrase in enumerate(keywords):\n",
        "        phrase_l = phrase.lower()\n",
        "        if \" \" in phrase_l and phrase_l in text_norm.lower():\n",
        "            placeholder = f\"__KEYWORDPHRASE{idx}__\"\n",
        "            placeholder_map[placeholder] = phrase_l\n",
        "            # Replace the phrase with placeholder (case-insensitive)\n",
        "            # To be more robust, use regular expressions for full matches\n",
        "            text_norm = re.sub(re.escape(phrase_l), placeholder, text_norm, flags=re.IGNORECASE)\n",
        "\n",
        "    words = text_norm.split()\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        wl = word.lower()\n",
        "        # If this is a placeholder or a single-keyword, keep it\n",
        "        if word in placeholder_map or wl in [kw.lower() for kw in keywords]:\n",
        "            filtered_words.append(word)\n",
        "        elif wl not in stop_words:\n",
        "            filtered_words.append(word)\n",
        "    # Restore multi-word phrases\n",
        "    result = ' '.join(filtered_words)\n",
        "    for placeholder, phrase in placeholder_map.items():\n",
        "        result = result.replace(placeholder, phrase)\n",
        "    return result\n",
        "\n",
        "# Remove very short and very long words\n",
        "def remove_short_and_long_words(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if len(word) > 2 and len(word) < 15]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def convert_lower(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.lower()\n",
        "\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_stop_words)\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_short_and_long_words)\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_punctuation)\n",
        "df[\"text_clean\"] = df[\"text_clean\"].apply(convert_lower)\n",
        "\n",
        "\n",
        "print(\"Head after removing stop words and short/long words:\")\n",
        "print(df.head())\n",
        "\n",
        "# Test it works\n",
        "test_text = \"The fire in the forest is a disaster that has caused a lot of damage.\"\n",
        "print(remove_stop_words(test_text))\n",
        "print(remove_short_and_long_words(test_text))\n",
        "print(remove_punctuation(test_text))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 4: Complete Preprocessing Pipeline\n",
        "\n",
        "**Goal**: Combine all cleaning steps into a comprehensive pipeline\n",
        "\n",
        "**Steps**:\n",
        "1. Create a main preprocessing function that:\n",
        "   - Applies all cleaning steps in order\n",
        "   - Handles edge cases (empty strings, very short text)\n",
        "   - Returns cleaned text\n",
        "2. Test the pipeline on sample data:\n",
        "   - Compare before/after examples\n",
        "   - Measure processing time\n",
        "   - Check for any issues\n",
        "\n",
        "**Hint**: Make the pipeline configurable with parameters for different cleaning strategies\n",
        "\n",
        "**Expected Output**: Robust preprocessing pipeline ready for the entire dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ''\n",
            "Processed: ''\n",
            "--------------------------------------------------\n",
            "Original: 'a'\n",
            "Processed: ''\n",
            "--------------------------------------------------\n",
            "Original: 'LOL!!!'\n",
            "Processed: 'lol'\n",
            "--------------------------------------------------\n",
            "Original: 'No disaster here.'\n",
            "Processed: 'disaster'\n",
            "--------------------------------------------------\n",
            "Original: 'RT @user: #BreakingNews Disaster at www.example.com!!'\n",
            "Processed: 'breakingnews disaster wwwexamplecom'\n",
            "--------------------------------------------------\n",
            "Original: 'FLOOD....flood.... Flooded.'\n",
            "Processed: 'floodflood flooded'\n",
            "--------------------------------------------------\n",
            "Original: '     '\n",
            "Processed: ''\n",
            "--------------------------------------------------\n",
            "Original: 'This is a test tweet with a reallysuperlongwordthatexceedslimits'\n",
            "Processed: 'test tweet'\n",
            "--------------------------------------------------\n",
            "Original: 'Fire! ðŸ”¥ðŸ”¥ðŸ”¥ Very bad situation... #emergency'\n",
            "Processed: 'fire ðŸ”¥ðŸ”¥ðŸ”¥ bad situation emergency'\n",
            "--------------------------------------------------\n",
            "Original: 'OMG ðŸ’¥'\n",
            "Processed: 'omg'\n",
            "--------------------------------------------------\n",
            "Original: 'The THE the tHe'\n",
            "Processed: ''\n",
            "--------------------------------------------------\n",
            "Original: 'The fire in the forest is a disaster that has caused a lot of damage ðŸ”¥ and OMG ðŸ’¥.'\n",
            "Processed: 'fire forest disaster caused lot damage omg'\n",
            "--------------------------------------------------\n",
            "Original: 'http://link.com Disaster!!!'\n",
            "Processed: 'disaster'\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# TODO 4: Complete preprocessing pipeline\n",
        "# Your implementation here\n",
        "def preprocess_text(text):\n",
        "    text = remove_url(text)\n",
        "    text = remove_mentions(text)\n",
        "    text = remove_hashtags(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = remove_short_and_long_words(text)\n",
        "    text = convert_lower(text)\n",
        "    return text\n",
        "\n",
        "# Test representative with all the preprocessing steps\n",
        "test_text = [\n",
        "    \"\",  # empty string\n",
        "    \"a\",  # single short word\n",
        "    \"LOL!!!\",  # punctuation and acronym\n",
        "    \"No disaster here.\",  # contains a stopword\n",
        "    \"RT @user: #BreakingNews Disaster at www.example.com!!\",  # contains RT, mention, hashtag, URL, punctuation\n",
        "    \"FLOOD....flood.... Flooded.\",  # repeated, case variants, punctuation\n",
        "    \"     \",  # whitespace only\n",
        "    \"This is a test tweet with a reallysuperlongwordthatexceedslimits\",  # very long word\n",
        "    \"Fire! ðŸ”¥ðŸ”¥ðŸ”¥ Very bad situation... #emergency\",  # emojis, punctuation, hashtag\n",
        "    \"OMG ðŸ’¥\",  # uppercase acronym and emoji\n",
        "    \"The THE the tHe\",  # case sensitivity\n",
        "    \"The fire in the forest is a disaster that has caused a lot of damage ðŸ”¥ and OMG ðŸ’¥.\",  # emojis, punctuation, hashtag\n",
        "    \"http://link.com Disaster!!!\",  # link + exclamation\n",
        "]\n",
        "# Apply preprocessing step to each test_text item and print the results\n",
        "for i, t in enumerate(test_text):\n",
        "    print(f\"Original: {repr(t)}\")\n",
        "    print(f\"Processed: {repr(preprocess_text(t))}\")\n",
        "    print('-'*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO 5: Apply Preprocessing and Save Results\n",
        "\n",
        "**Goal**: Process the entire dataset and save cleaned data\n",
        "\n",
        "**Steps**:\n",
        "1. Apply preprocessing pipeline to training data\n",
        "2. Apply same pipeline to test data\n",
        "3. Save cleaned data to `data/interim/` folder\n",
        "4. Compare before/after statistics\n",
        "5. Document any data lost during preprocessing\n",
        "\n",
        "**Expected Output**: Clean datasets ready for tokenization\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Transformers Enhancement\n",
        "\n",
        "*After completing Phase 1, consider these enhancements:*\n",
        "\n",
        "- Use HuggingFace tokenizers for consistent preprocessing\n",
        "- Leverage pre-trained tokenization (BERT, RoBERTa)\n",
        "- Compare custom preprocessing vs. transformer tokenization\n",
        "- Analyze preprocessing impact on different model architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns are the same\n",
            "âœ… Test columns match train columns (except 'target'). Train columns: 15, test columns: 14\n",
            "Missing columns: ['target']\n",
            "Target column is missing, which is fine as it is not needed for the test dataset\n",
            "Index(['id', 'keyword', 'location', 'text', 'target', 'has_hashtag',\n",
            "       'has_mention', 'has_url', 'number_urls', 'number_hashtags',\n",
            "       'number_mentions', 'text_clean', 'emojis_text', 'word_count',\n",
            "       'word_count_difference'],\n",
            "      dtype='object')\n",
            "Index(['id', 'keyword', 'location', 'text', 'has_hashtag', 'has_mention',\n",
            "       'has_url', 'number_urls', 'number_hashtags', 'number_mentions',\n",
            "       'word_count', 'text_clean', 'word_count_difference', 'emojis_text'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# TODO 5: Apply preprocessing and save results\n",
        "# Your implementation here\n",
        "# Create a column for potential debugging purposes so it is present in the cleaned dataset\n",
        "# One potential way to check that the preprocessing is working as expected is to compare the word count before and after preprocessing\n",
        "df[\"word_count_difference\"] = df[\"word_count\"] - df[\"text_clean\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"../data/interim/train_cleaned.csv\", index=False)\n",
        "\n",
        "df_test = pd.read_csv(\"../data/raw/test.csv\")\n",
        "# Add same column for test dataset ['id', 'keyword', 'location', 'text', 'target', 'has_hashtag','has_mention', 'has_url', 'number_urls', 'number_hashtags', 'number_mentions', 'text_clean', 'emojis_text', 'word_count', 'word_count_difference']\n",
        "\n",
        "df_test[\"has_hashtag\"] = df_test[\"text\"].str.contains(r\"#\")\n",
        "df_test[\"has_mention\"] = df_test[\"text\"].str.contains(r\"@\")\n",
        "df_test[\"has_url\"] = df_test[\"text\"].str.contains(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "df_test[\"number_urls\"] = df_test[\"text\"].str.count(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
        "df_test[\"number_hashtags\"] = df_test[\"text\"].str.count(r\"#\")\n",
        "df_test[\"number_mentions\"] = df_test[\"text\"].str.count(r\"@\")\n",
        "df_test[\"word_count\"] = df_test[\"text\"].apply(lambda x: len(x.split()))\n",
        "df_test[\"text_clean\"] = df_test[\"text\"].apply(preprocess_text)\n",
        "df_test[\"word_count_difference\"] = df_test[\"word_count\"] - df_test[\"text_clean\"].apply(lambda x: len(x.split()))\n",
        "df_test[\"emojis_text\"] = df_test[\"text\"].apply(get_emoji_text_or_none)\n",
        "\n",
        "if df.columns.difference(df_test.columns).empty:\n",
        "    print(\"Columns are not the same\")\n",
        "else:\n",
        "    print(\"Columns are the same\")\n",
        "\n",
        "# Check that train and test have the same columns except for \"target\" (test should not have it)\n",
        "expected_columns = [col for col in df.columns if col != \"target\"]\n",
        "missing_from_test = [col for col in expected_columns if col not in df_test.columns]\n",
        "extra_in_test = [col for col in df_test.columns if col not in expected_columns]\n",
        "\n",
        "if not missing_from_test and not extra_in_test:\n",
        "    print(f\"âœ… Test columns match train columns (except 'target'). Train columns: {len(df.columns)}, test columns: {len(df_test.columns)}\")\n",
        "else:\n",
        "    print(f\"âŒ Columns mismatch. Missing from test: {missing_from_test}, Extra in test: {extra_in_test}\")\n",
        "\n",
        "# Checking which columns are missing\n",
        "missing_columns = []\n",
        "\n",
        "for col in df.columns:\n",
        "    if col not in df_test.columns:\n",
        "        missing_columns.append(col)\n",
        "\n",
        "print(f\"Missing columns: {missing_columns}\")\n",
        "\n",
        "if \"target\" in missing_columns:\n",
        "    print(\"Target column is missing, which is fine as it is not needed for the test dataset\")\n",
        "else:\n",
        "    print(\"Target column is present, which is not fine as it is not needed for the test dataset\")\n",
        "\n",
        "# Save the cleaned test dataset\n",
        "df_test.to_csv(\"../data/interim/test_cleaned.csv\", index=False)\n",
        "\n",
        "print(df.columns)\n",
        "print(df_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: HuggingFace Tokenizers Comparison\n",
        "\n",
        "**Goal**: Compare custom preprocessing with HuggingFace tokenizers\n",
        "\n",
        "**Steps**:\n",
        "1. Install and import HuggingFace transformers\n",
        "2. Load BERT tokenizer and compare outputs\n",
        "3. Analyze differences between custom vs. transformer tokenization\n",
        "4. Document pros/cons of each approach\n",
        "\n",
        "**Expected Output**: Understanding of when to use custom vs. pre-trained tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 2: HuggingFace Tokenizers Comparison\n",
        "# Your implementation here\n",
        "\n",
        "# Install transformers if not already installed\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample texts for comparison\n",
        "sample_texts = [\n",
        "    \"Fire! ðŸ”¥ðŸ”¥ðŸ”¥ Very bad situation... #emergency\",\n",
        "    \"RT @user: #BreakingNews Disaster at www.example.com!!\",\n",
        "    \"The fire in the forest is a disaster that has caused a lot of damage ðŸ”¥\",\n",
        "    \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\",\n",
        "    \"Forest fire near La Ronge Sask. Canada\"\n",
        "]\n",
        "\n",
        "print(\"=== COMPARISON: Custom Preprocessing vs. BERT Tokenizer ===\\n\")\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Original: {repr(text)}\")\n",
        "    \n",
        "    # Custom preprocessing\n",
        "    custom_cleaned = preprocess_text(text)\n",
        "    print(f\"Custom cleaned: {repr(custom_cleaned)}\")\n",
        "    \n",
        "    # BERT tokenization\n",
        "    bert_tokens = tokenizer.tokenize(text)\n",
        "    bert_cleaned = \" \".join(bert_tokens)\n",
        "    print(f\"BERT tokens: {bert_tokens}\")\n",
        "    print(f\"BERT cleaned: {repr(bert_cleaned)}\")\n",
        "    \n",
        "    # Compare lengths\n",
        "    custom_words = len(custom_cleaned.split()) if custom_cleaned else 0\n",
        "    bert_words = len(bert_tokens)\n",
        "    \n",
        "    print(f\"Custom word count: {custom_words}\")\n",
        "    print(f\"BERT token count: {bert_words}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze tokenization differences and create comparison statistics\n",
        "\n",
        "def analyze_tokenization_differences(df_sample, tokenizer):\n",
        "    \"\"\"\n",
        "    Compare custom preprocessing vs BERT tokenization on a sample of data\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'custom_word_counts': [],\n",
        "        'bert_token_counts': [],\n",
        "        'length_ratios': [],\n",
        "        'samples': []\n",
        "    }\n",
        "    \n",
        "    # Sample a subset for analysis (first 100 rows)\n",
        "    sample_df = df_sample.head(100)\n",
        "    \n",
        "    for idx, row in sample_df.iterrows():\n",
        "        original_text = row['text']\n",
        "        custom_cleaned = preprocess_text(original_text)\n",
        "        bert_tokens = tokenizer.tokenize(original_text)\n",
        "        \n",
        "        custom_words = len(custom_cleaned.split()) if custom_cleaned else 0\n",
        "        bert_tokens_count = len(bert_tokens)\n",
        "        \n",
        "        results['custom_word_counts'].append(custom_words)\n",
        "        results['bert_token_counts'].append(bert_tokens_count)\n",
        "        \n",
        "        if custom_words > 0:\n",
        "            ratio = bert_tokens_count / custom_words\n",
        "            results['length_ratios'].append(ratio)\n",
        "        \n",
        "        results['samples'].append({\n",
        "            'original': original_text[:50] + \"...\",\n",
        "            'custom_count': custom_words,\n",
        "            'bert_count': bert_tokens_count\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run analysis\n",
        "print(\"=== TOKENIZATION ANALYSIS ON SAMPLE DATA ===\\n\")\n",
        "tokenization_results = analyze_tokenization_differences(df, tokenizer)\n",
        "\n",
        "# Calculate statistics\n",
        "avg_custom = sum(tokenization_results['custom_word_counts']) / len(tokenization_results['custom_word_counts'])\n",
        "avg_bert = sum(tokenization_results['bert_token_counts']) / len(tokenization_results['bert_token_counts'])\n",
        "avg_ratio = sum(tokenization_results['length_ratios']) / len(tokenization_results['length_ratios'])\n",
        "\n",
        "print(f\"Average custom preprocessing word count: {avg_custom:.2f}\")\n",
        "print(f\"Average BERT token count: {avg_bert:.2f}\")\n",
        "print(f\"Average BERT/Custom ratio: {avg_ratio:.2f}\")\n",
        "print(f\"BERT typically produces {avg_ratio:.1f}x more tokens than custom preprocessing\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\n=== SAMPLE COMPARISONS ===\")\n",
        "for i, sample in enumerate(tokenization_results['samples'][:5]):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Text: {sample['original']}\")\n",
        "    print(f\"  Custom words: {sample['custom_count']}, BERT tokens: {sample['bert_count']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Recommendations\n",
        "\n",
        "### Custom Preprocessing vs. HuggingFace Tokenizers\n",
        "\n",
        "**Custom Preprocessing Advantages:**\n",
        "- âœ… **Domain-specific**: Preserves disaster-related keywords\n",
        "- âœ… **Controllable**: Full control over cleaning steps\n",
        "- âœ… **Interpretable**: Clear understanding of each transformation\n",
        "- âœ… **Efficient**: Smaller vocabulary, faster processing\n",
        "\n",
        "**HuggingFace Tokenizers Advantages:**\n",
        "- âœ… **Pre-trained**: Optimized for specific models (BERT, RoBERTa)\n",
        "- âœ… **Robust**: Handles edge cases and special tokens\n",
        "- âœ… **Standardized**: Consistent across different projects\n",
        "- âœ… **Subword handling**: Better handling of OOV words\n",
        "\n",
        "**Recommendations:**\n",
        "1. **Phase 1 (PyTorch Fundamentals)**: Use custom preprocessing for learning\n",
        "2. **Phase 2 (Transformers)**: Use HuggingFace tokenizers for better performance\n",
        "3. **Hybrid Approach**: Combine custom cleaning with pre-trained tokenizers\n",
        "\n",
        "### Next Steps:\n",
        "- Move to `02_vocab_and_dataloader.ipynb` for vocabulary building\n",
        "- Use cleaned datasets from `data/interim/` folder\n",
        "- Implement custom PyTorch data loaders with your cleaned text\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
